---
title: "Exam Preperation"
output: rmarkdown::github_document
---

## 1. 데이터 이해 (8 + 2)

#### 데이터 베이스의 특징

- 데이터베이스는 여러 사용자가 서로 다른 목적으로 데이터를 공동으로 이용할 수 있도록 구성되어 있다
- 데이터베이스는 통합된 데이터(integrated data)다. 동일한 내용의 데이터가 중복되어 있지 않음을 의미
- 데이터베이스는 변화하는 데이터로 데이터의 삽입, 삭제, 갱신을 한다고 하더라도 항상 현재의 정확한 데이터를 유지해야 한다
- 데이터베이스는 컴퓨터가 접근할 수 있는 저장매체에 저장되어있다

#### 데이터 베이스의 특성

- 데이터베이스는 검색기능을 가지고 있으므로 다양한 방법으로 필요한 정보를 검색할 수 있다
- 이용자의 요구에 따라 신속하게 획득하고, 원하는 정보를 경제적으로 찾아 낼 수 있다
- 방대한 양을 체계적으로 축적하고 새로운 내용 추가나 갱신이 용이
- 정보처리, 검색 관리 소프트웨어 등 네트워크 발전 기술 견인
- 인프라로서 특성을 가지고 있어 경제, 산업, 사회 활동의 효율성을 제고하고 국민의 편의 증진

#### DIKW 피라미드

**(4) Wisdom(지혜):** understanding, integrated, actionable

- 근본 원리에 대한 깊은 이해를 바탕으로 도출되는 아이디어
  - A마트의 다른 상품들도 B마트보다 쌀 것이라고 판단
  
**(3) Knoweldge(지식)**: contextual, synthesized, learning

- 상호 연결된 정보 패턴을 이해하여 이를 토대로 예측한 결과물
  - 상대적으로 저렴한 A마트에서 연핀을 사야겠다
  
**(2) Information(정보)**: useful, organized, structured

- 데이터의 가공 및 상관관계 간 이해를 통해 패턴을 인식하고 의미를 부여
  - A마트의 연필 가격이 더 싸다
  
**(1) Data(데이터)**: signals, know nothing

- 존재 형식 불문, 타 데이터와 상관관계가 없는 가공 전의 순수한 수치나 기호
  - A 마트 100원, B 마트 200원에 연필을 판매
  
#### 데이터 사이언티스트의 요구 역량

- **Hard Skill**
  - 빅데이터에 대한 이론적 지식: 관련 기법에 대한 이해와 방법론 습득
  - 분석 기술에 대한 숙련: 최적의 분석 설계 및 노하우 축적

- **Soft Skill**
  - 통찰력 있는 분석: 창의적 사고, 호기심, 논리적 비판
  - 설득력 있는 전달: 스토리텔링, Visualization 
  - 다분야 간 협력: Communication
  
#### 빅데이터가 만들어 내는 본질적인 변화

- 빅데이터 분석의 경제성 제공
  - 클라우드 컴퓨팅으로 수집 비용 감소
  
#### 빅데이터 시대의 위기 요인  

가. 사생활 침해 -> 동의에서 책임으로

- **내용:** 사생활 침해를 넘어 사회/경제적 위협으로 변형 가능  
- **통제방안:** 익명화(Anonymization), 개인정보 제공자 동의(에서) -> 사용자 책임(으로)  

나. 책임 원칙 훼손 -> 결과 기반 책임 원칙 고수  

- **내용:** 분석 대상 사람들, 예측 알고리즘의 희생양, 명확한 행동 없이 잠재적 위협으로 책임 짐
- **통제방안:** 기존의 책임원칙 강화

다. 데이터 오용 -> 알고리즘 접근 허용

- **내용:** 높은 정확도 불구, 항상 정확 X
- **통제방안:** 데이터 알고리즘에 대한 접근권 허용 및 객관적 인증방안 도입

#### 사물인터넷 시대

- 모든 것의 데이터화(datafication)

#### 데이터사이언스와 인문학

- **단순세계화 -> 북잡한 세계화:** 다양성과 각 사회의 정체성과 그 맥락, 관계, 연결성, 창조성이 키워드로 대두
- **비즈니스의 중심 - 제품생산 -> 서비스:** 제품이 고장나더라도 뛰어난 고객 서비스를 제공해주는 것이 중요
- **경제와 산업의 논리 - 생산 -> 시장창조:** 새로운 현지화 패러다임에 근거한 시장 창조
- 한계
  - 아무리 정량적인 분석이라도 모든 분석은 가정에 근거
  - 분석과정에서는 가정 등 인간의 해석이 개입되는 단계를 반드시 거쳐야 함
  - 분석결과가 의미하는 바는 사람에 따라 전혀 다른 해석과 결론을 내릴 수 있음


#### Data에 관련한 기술

- 개인정보 비식별 기술
  - 데이터 마스킹
    - 데이터의 길이, 유형, 형식과 같은 속성을 유지한 채, 새롭고 읽기 쉬운 데이터를 익명으로 생성
    - 개인의 사생활 침해를 방지하고 통계 응답자의 비밀사항은 보호하면서 통계자료의 유용성을 최대한 확보 할 수 있는 데이터 변환 법
  - 가명처리
  - 총계처리
  - 데이터값 삭제
  - 데이터 범주화

- 무결성과 레이크
  - 데이터 무결성 (Data Integrity)
    - 데이터베이스 내의 데이터에 대한 정확한 일관성, 유효성, 신뢰성을 보장하기 위해 데이터 변경/수정 시 여러 가지 제한을 두어 데이터의 정확성을 보증하는 것
  - 데이터 레이크 (Data Lake)
    - 수 많은 정보 속에서 의미있는 내용을 찾기 위해 방식에 상관없이 데이터를 저장하는 시스템
    - 대용량의 정형 및 비정형 데이터를 저장할 뿐만 아니라 접근도 쉽게 할 수 있는 대규모의 저장소를 의미

#### 데이터양의 단위 x 1,024

- 바이트 (B) -> 킬로바이트 (KB) -> 메가바이트 (MB) -> 기가바이트 (GB) -> 테라바이트 (TB) -> 페타바이트 (PB) -> 엑사바이트 (EB) -> 제타바이트 (ZB) -> 요타바이트 (YB)







----------------------

## 2. 데이터 분석 기획 (8 + 2)

#### 분석 주제의 4가지 유형

\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  **분석의 대상 (What)**  

\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ Known \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  Un-Known  
**분석의 방법 (How)** \ \ \ \ \ \ \ \ \ \ **최적화 (Optimization)** \ \ \ \ \ \ \ **통찰 (Insight)** \ \ \ \ \ \ \ \ \ \ \ \ Known  
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ **솔루션 (Solution)** \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ **발견 (Discovery)** \ \ \ \ \ \ Un-Known  

#### 분석 기획시 고려사항

- 가용한 데이터 (Available Data)
- 적절한 유즈케이스
- 장애요소들에 대한 사전계획 수립

#### 분석 방법론 구성요소

- 상세한 절차 (Procedure)
- 방법 (Methods)
- 도구와 기법 (Tools & Techniques)
- 템플릿과 산출물 (Templates & Outputs)

#### 분석 방법론 장애요소

- 고정관념 (Stereotype)
- 편향된 생각 (Bias)
- 프레이밍효과 (Framing Effect)
  - 문제의 표현 방식에 따라 동일한 사건이나 상황임에도 불구하고 개인의 판단이나 선택이 달라질 수 있는 현상


#### 분석 방법론

- KDD (Knowledge Discovery in Database)
  - 데이터 선택 (Selection)
  - 데이터 전처리 (Preprocessing)
  - 데이터 변환 (Transformation)
  - 데이터 마이닝 (Data Mining)
  - 데이터 마이닝 결과 평과 (Interpretation/Evaluation)

- CRISP-DM (Cross Industry Standard Process for Data Mining)
  - 총 6단계로 구성되어 있으며, 각 단계는 단방향(폭포수)으로 구성되어 있지 **않고** 단계 간 피드백을 통하여 단계별 완성도를 높이게 되어 있다
  - 업무이해 (Business Understanding)
  - 데이터 이해 (Data Understanding)
  - 데이터 준비 (Data Preparation)
  - 모델링 (Modeling)
  - 평가 (Evaluation)
  - 전개 (Deployment)
  

#### 분석 과제 발굴 방법론

- 하향식 접근법
  - Problem Discovery -> Problem Definition -> Solution Search -> Feasibility Study
  - Why 관점
  - [1] 문제 탐색 (Problem Discovery)
    - 문제를 해결 함으로써 발생하는 가치에 중점 
    - 무엇을(What) 어떤 목적으로(Why) 수행해야 하는지에 대한 관점
    - 비즈니스 모델 기반 문제 탐색
    - 분석 기회 발굴의 범위 확장
    - 외부 참조 모델 기반 문제 탐색
    - 분석 유즈 케이스 (Analytics Use Case)
  - [2] 문제 정의 (Problem Definition)
    - 식별된 비즈니스 문제를 데이터의 문제로 변환하여 정의하는 단계
    - (How)을 정의하기 위한 데이터분석의 문제로의 변환을 수행
  - [3] 해결 방안 탐색 (Solution Search)
    - 정의된 데이터 분석 문제를 해결 하기 위한 다양한 방안이 모색
  - [4] 타당성 검토 (Feasibility Study)
    - 경제적 타당성
      - 비용대비 편익 분석 관점의 접근
    - 데이터 및 기술적 타당성
      - 데이터 존재 여부, 분석 시스템 환경, 그리고 분석 역량이 필요
- 상향식 접근법
  - What 관점
- 디자인적 사고
  - 상향식 접근 방식의 발산단계와 도출된 옵션을 분석하고 검증하는 하향식 접근 방식의 수렴단계를 반복하여 과제를 발굴하는 방법


- **[1] 문제 탐색 (Problem Discovery)** **<span style="color: red;">중요!!</span>**

  - 과제 발굴 단계에서는 세부적인 구현 및 솔루션에 초첨을 맞추는게 아니라, **문제를 해결** 함으로써 **발생하는 가치에 중점** 을 두는 것이 중요
  - **비즈니스 모델 기반 문제 탐색**
    - 비즈니스모델 켄버스의 9가지 블록을 단순화 하여 **업무, 제품, 고객** 단위로 문제를 발굴, 이를 관리하는 **규제와 감사, 지원 인프라** 영역으로 기회를 추가 도출
    - **업무 (Operation), 제품 (Product), 고객 (Customer)**
    - **규제와 감사 (Regulation & Audit), 지원 인프라 (IT & Human Resource)**
    - 새로운 유형의 분석 기회 및 주제 발굴을 수행해야 함
  - **분석 기회 발굴의 범위 확장**
    - 거시적 관점의 메가트랜드
      - STEEP 영역으로 폭넓게 기획 탐색 수행
      - **사회 (Social), 기술 (Technological), 경제 (Economic), 환경 (Environmental), 정치 (Political)**
    - 경쟁자 확대 관점
      - **대체제 (Substitute), 경쟁자 (Competitor), 신규 진입자 (New Entrant)**
    - 시장의 니즈 탐색 관점
      - **고객 (Customer), 채널 (Channel), 영향자들 (Influencer)**
    - 역량의 재해석 관점
      - **내부 역량 (Competency), 파트너와 네트워크 (Partners & Network)**
  - **외부 참조 모델 기반 문제 탐색**
    - 유사/동종 사례 벤치마킹을 통한 분석기회 발굴은 제공되는 산업별, 업무 서비스별 분석 테마 후보 그룹을 통해 **Quick & Easy** 방식으로 필요한 분석기회가 무엇인지에 대한 아이디어를 얻고, 기업에 적용할 분석테마 후보 목록을 워크숍 형태의 **브레인스토밍(Brain storming)** 을 통해 빠르게 도출하는 방법
    - 현재 환경에서는 데이터를 활용하지 않은 업종 및 업무 서비스가 사실상 존재하지 않기 때문에 업무에 활용되는 사례들을 발굴하여 자사의 업종 및 업무 서비스에 적용할 수 있음
  - **분석 유즈 케이스 (Analytics Use Case)**
    - 풀어야 할 문제에 대한 상세한 설명 및 해당 문제를 해결했을 때 발생하는 효과를 명시함으로써 향후 데이터 분석 문제로의 전환 및 적합성 평가에 활용
    
#### 분석과제 주요 관리 영역 5가지

- Data Size
- Data Complexity
- Speed
- Analytic Complexity
- Accuracy & Precision

#### 프로젝트의 관리 방안

- 주요 관리 항목
  - 범위, 시간, 원가, 품질, 통합, 조달, 자원, 리스크, 의사소통, 이해관계자
  
#### 과제 관리 프로세스

- 과제 발굴 단계
  - [1] 분석 아이디어 발굴
  - [2] 분석과제 후보제안 (과제후보 pool)
  - [3] 분석과제 확정 (전사분석 조직) -> 확정 분석 과제와 후보 과제 모두 풀로 관리

- 과제 수행 단계
  - [4] 팀구성
  - [5] 분석과제 식별/실행 (전사분석 조직의 과제 수행 지원)
  - [6] 분석과제 진행관리 (전사분석 조직)
  - [7] 결과 공유/개선 (과제결과 pool)
  
#### 데이터 분석 수준 진단

- 분석 준비도 (Readiness)
  - 분석 업무, 분석 인력/조직, 분석 기법, 분석 데이터, 분석 문화, 분석 인프라
  - 분석 업무 파악
    - 발생한 사실 분석 업무
    - 예측분석 업무
    - 시뮬레이션 분석 업무
    - 최적화 분석 업무
    - 분석 업무 정기적 개선
  - 분석 데이터
    - 분석업무를 위한 데이터 충분성
    - 분석업무를 위한 데이터 신뢰성
    - 분석업무를 위한 데이터 적시성
    - 비구조적 데이터 관리
    - 외부 데이터 활용 체계
    - 기준데이터 관리(MDM)
  
#### 데이터 분석 성숙도 

- 도입
  - 분석을 시작하여 환경과 시스템을 구축
- **활용**
  - 분석 결과를 실제 업무에 적용
- **확산**
  - 도입되어 지속적인 확산 필요
- 최적화
  - 분석을 진화시켜서 혁신 및 성과 향상에 기여
  
#### 데이터 거버넌스 체계 수립

- 데이터 저장소 (Repository) 관리
  - 데이터 구조 변경에 따른 **사전 영향 평가** 도 수행되어야 효율적인 활용이 가능하다
  
#### 데이터 조직 및 인력방안 수립

- **분석을 위한 3가지 조직 구조** \ \ \ \ \ \ \ \ \ \ **<span style="color: red;">빈도 높음!</span>**
  - **집중형**
    - 전사 분석업무를 별도의 분석전담 조직에서 담당
    - 전략적 중요도에 따라 분석조직이 우선순위를 정해서 진행 가능
    - 현업 업무부서의 분석업무와 이중화/이원화 가능성 높음
  - **기능형**
    - 일반적인 분석 수행 구조
    - 별도 분석조직이 없고 해당 업무 부서에서 분석 수행
    - 전사적 핵심분석이 어려우며, 부서 현황 및 실적 통계 등 과거 실적에 국한된 분석 수행 가능성 높음
  - **분산형**
    - 분석조직 인력들을 현업부서로 직접 배치하여 분석업무 수행
    - 전사차원의 우선순위 수행
    - 분석결과에 따른 신속한 Action 가능
    - 베스트 프랙티스 공유 가능
    - 부서 분석업무와 역할 분담 명확히 해야함 (-> 업무과다 이원화 가능성)





------------------

## 3. 데이터 분석 (24 + 6)

#### 표본 추출 방법

- [1] 단순랜덤 추출법(simple random sampling)
  - 각 샘플에 번호를 부여하여 임의의 n개를 추출하는 방법으로 각 샘플은 선택될 확률이 동일
- [2] 계통추출법(systematic sampling)
  - 임의 위치에서 매 k번째 항목을 추출하는 방법
- [3] 집락추출법(cluster random sampling)
  - 모집단을 군집으로 구분하고 선정된 군집의 원소를 모두 샘플로 추출
  - 군집을 구분하고 군집별로 단순랜덤 추출법을 수행한 후, 모든 자료를 활용하거나 샘플링하는 방법
- [4] 층화추출법(stratified random sampling)
  - 모집단을 몇 개의 집단으로 구분, 각 집단의 크기와 분산 고려하여 각 집단마다 샘플 추출
  - 각 계층을 고루 대표할 수 있도록 표본을 추출

#### 측정(measurement)

- 질적척도 (범주형자료, 숫자들의 크기 차이가 계산되지 않는 척도)
    - **명목척도 (Nominal Scale)**
      - 측정 대상이 어느 **집단** 에 소속하는지 분류할 때 사용 (성별, 출생지 구분)
    - **순서척도(Ordinal Scale)**
      - 측정 대상의 **서열관계** 를 관측하는 척도 (만족도, 선호도, 학년, 신용등급)
- 양적척도(수치형자료, 숫자들의 크기 차이를 계산할 수 있는 척도)  
    - **구간척도(등간척도, Interval Scale)**
      - 측정 대상이 갖고 있는 **속성의 양** 을 측정하는 것으로 구간이나 구간 사이의 **간격이 의미가 있는** 자료 (온도, 지수)
    - **비율척도(Ratio Scale)**
      - 간격(차이)에 대한 비율이 의미를 가지는 자료
      - **절대적 기준인 0이 존재** 하고 **사칙연산이 가능** 하며 제일 많은 정보를 가지는 척도 (무게, 나이, 시간, 거리)
      
#### 확률 분포

- [1] 이산형 확률변수 (0 or 1)
  - 베르누이 확률분포(Bernoulli distribution)
  - 이항분포(Binomial distribution)
  - 기하분포(Geometric distribution)
  - 다항분포(Multinomial distribution)
  - 포아송분포(Poisson disbritubtion)
- [2] 연속형 확률변수
  - 균일분포(일양분포, Uniform distribution)
  - 정규분포(Normal distribution)
  - 지수분포(Exponential distribution)
    - 어떤 사건이 발생할 때까지 경과 시간에 대한 연속확률분포이다
  - t-분포(t-distribution)
    - 두 집단의 평균이 동일 한지
  - 카이제곱 X^2 분포 (Chi-square distribution)
    - 표본의 분산이 카이제곱 분포를 따름
    - 두 집단 간의 동질성 검정
  - F-분포(F-distribution)
    - 두 집단간 분산의 동일성 검정
    
#### 최적 회귀 방식

- 단계적 변수선택(Stepwise Variable Selection)
  - 전진선택법(forward selection)
    - 절편만 있는 상수모형으로부터 시작해 중요하다고 생각되는 설명변수부터 차례로 모형에 추가한다
  - 후진제거법(backward elimination)
    - 독립변수 후보 모두를 포함한 모형에서 출발해 가장 적은 영향을 주는 변수부터 하나씩 제거하면서 더 이상 제거할 변수가 없을 때의 모형을 선택한다
  - 단계선택법(stepwise method)
    - 전진선택법에 의해 변수를 추가하면서 새롭게 추가된 변수에 기인해 기존 변수의 중요도가 약화되면 해당병수를 제거하는 등 단계별로 추가 또는 제거되는 변수의 여부를 검토해 더 이상 없을 때 중단한다
  - 최적선택법이라는 방식은 존재 X
  
#### 비모수 검정

- 자료가 추출된 모집단의 분포에 대한 아무 제약을 가하지 않고 검정을 실시 하는 방법
- 부호검정(sign test), 윌콕슨의 순위합검정(rank sum test), 윌콕슨의 부호순위합검정(Wilcoxon signed rank test), 만-위트니의 U 검정, 런검정(run test), 스피어만의 순위상관계수

#### 시계열 분석

- 정상성(stationary)
  - 오차의 분포가 정규분포를 따르는 것
  - 평균이 일정
    - 모든 시점에 대해 일정
    - 일정하지 않을 경우 차분(difference)를 통해 정상화
  - 분산이 일정
    - 시점에 의존 X
    - 일정하지 않을 경우 변환(transformation)을 통해 정상화
    - 정상성을 만족한다는 것은 분산이 시점에 의존하지 않는다는 것
  - 공분산
    - 시차에만 의존
    - 특정 시점 t, s 에는 의존 X
    
#### 시계열 모형

- 자기회기 모형 (Autoregressive(AR) model)
- 이동평균 모형(MA 모형, Moving Average model)
- 자귀회귀누적이동평균 모형 (ARIMA(p, d, q) 모형, Auto-Regressive Integrated Moving Average model)
- 분해 시계열
  - 경향(추세) 요인: 자료가 오르거나 내리는 추세, 선형, 이차식 형태, 지수적 형태 등
  - 계절요인: 요일, 월, 사계절 각 분기에 의한 변화 등 고정된 주기에 따라 자료가 변하는 경우
  - 순환요인: 경제적이나 자연적인 이유 없이 알려지지 않은 주기를 가지고 변화하는 자료
  - 불규칙요인: 위의 세 가지 요인으로 설명할 수 없는 오차에 해당하는 요인

#### 다차원 척도법

- 여러 대상 간의 거리가 주어져 있을 때, 대상들을 동일한 상대적 거리를 가진 실수 공간의 점들로 배치시키는 방법

#### 데이터 마이닝

- 분류분석 (Classification): rpart, rpartOrdinal, randomForest, party, marginTree, MapTree
  - 로지스틱 회귀분석 (Logistic Regression)
  - 의사결정나무 (Decision Tree)
  - 앙상블 분석 (Ensemble Analysis)
    - 배깅
    - 부스팅
    - 랜덤 포레스트(random forest)
  - 인공신경망 분석 (Artificial Neural Network Analysis)
- 군집분석 (Cluster Analysis)
  - 거리(연속형)
    - 유클리디안 거리(Euclidean distance)
      - sqrt{ (A1 - B1)^2 + (A2 - B2)^2 }
    - 표준화 거리(Statistical distance)
    - 마할라노비스(Mahalanobis) 거리
  - 거리(범주형)
    - 자카드 거리/계수
    - 코사인 거리/계수
  - 계층적
    - 최단 연결법 (single linkage, nearest neighbor)
    - 최장 연결법 (complete linkage, farthest neighbor)
    - 평균 연결법 (average linkage)
    - 와드 연결법 (ward linkage)
      - 군집내의 오차제곱합(SST) 에 기초하여 군집을 수행한다
  - 비계층적
    - k-평균 군집분석(k-means clustering)
  - 혼합 분포 군집 (mixture distribution clustering)
  - SOM (Self Organizing Map)
- 연관분석 (Association Analysis)
  - 지지도(support)
    - <img src="https://render.githubusercontent.com/render/math?math=$P(A \cap B) = (A \cap B) / total$">
  - 신뢰도(confidence)
    - <img src="https://render.githubusercontent.com/render/math?math=$P(A \cap B) / P(A) = support /P(A)$">
  - 향상도(lift)
    - <img src="https://render.githubusercontent.com/render/math?math=$P(B|A)/P(B) = P(A \cap B) / (P(A)P(B)) = confidence/P(B)$">
    - 연관규칙의 측정 지표 중 도출된 규칙의 우수성을 평가하는 기준
    - 두 품목의 상관관계를 기준으로 도출된 규칙의 예측력을 평가하는 지표
  - 순차 분석 (Sequence Analysis)

#### 분류분석 (Classification)

- 의사결정 나무
  - 정지규칙
    - 더 이상 분기가 되지 않고 현재의 마디가 끝마디가 되도록 하는 규칙
  - <장점>
    - 결과를 누구에게나 설명하기 용이하다
    - 모형을 만드는 방법이 계산적으로 복잡하지 않다
    - 대용량 데이터에서도 빠르게 만들 수 있다
    - 비정상 잡음 데이터에 대해서도 민감함이 없이 분류할 수 있다
    - 한 변수와 상관성이 높은 다른 불필요한 변수가 있어도 크게 영향을 받지 않는다
    - 설명변수나 목표변수에 수치형변수와 범주형변수를 모두 사용 가능하다
    - 모형 분류 정확도가 높다
  - <단점>
    - 새로운 자료에 대한 과대적합이 발생할 가능성이 높다
    - 분류 경계선 부근의 자료값에 대해서 오차가 크다
    - 설명변수 간의 중요도를 판단하기 쉽지 않다

#### 군집분석 (Cluster Analysis)
  - 계층적
    - 군집 개수를 미리 지정하지 않아도 되며 탐색적 분석에 사용됨
    - 동일한 계산법 적용 시, 항상 동일한 결과가 나옴
  - 비계층적(k-means)
    - 한 개체가 처음 속한 군집에서 다른 군집으로 이동 및 재배 가능
    - 초기값에 대한 의존이 큼. 값에 따라 군집이 달라짐
    - 장점
        - 알고리즘이 단순, 빠르게 수행되어 분석 방법 적용이 용이
        - 계층적 군집분석에 비해 많은 양의 데이터를 다룸
        - 내부 구조에 대한 사전정보가 없어도 의미있는 자료구조를 찾음
        - 다양한 형태의 데이터에 적용이 가능
    - 단점
        - 군집의 수, 가중치와 거리 정의가 어려움
        - 사전에 주어진 목적이 없으므로 결과 해석이 어려움
        - 잡음이나 이상값의 영향을 많이 받음
        - 볼록한 형태가 아닌 (non-convex) 군집이 (예를 들어 U형태의 군집) 존재할 경우에는 성능이 떨어짐
        - 초기 군집수 결정에 어려움 있음

#### 데이터 마이닝을 위한 데이터 분할

- 홀드아웃(hold-out) 방법
  - 주어진 데이터를 랜덤하게 두 개의 데이터로 구분하여 사용하는 방법으로 주로 학습용(training data)과 시험용(test data)로 분리하여 사용한다
- 교차확인(cross-validation) 방법
  - 주어진 데이터를 k개의 하부집단으로 구분하여, k - 1개의 집단을 학습용으로 나머지는 하부집단으로 검증용으로 설정하여 학습한다


#### 데이터 마이닝 성과 분석

- ROC Curve
  - 가로축: 1 - 특이도(specificity)     
  - 세로축: 민감도(Sensitivity)
  - ROC 곡선 아래의 면적을 의미하는 AUROC(Area Under ROC) 값이 크면 클수록(1에 가까울수록) 모형의 성능이 좋다고 평가