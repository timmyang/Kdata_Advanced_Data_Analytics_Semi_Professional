---
title: "Part 5 Summary"
output: rmarkdown::github_document
---

# 데이터 마이닝 (Data Mining)

#### 데이터 분할

- 구축용 (training data, 50%)
- 검정용 (validation data, 30%)
- 시험용 (test data, 20%)
- 데이터의 양이 충분하지 않거나, 입력 변수에 대한 설명이 부족한 경우
  - 홀드아웃(hold-out) 방법
    - 데이터를 무작위로 두집단(training, test)으로 분리하여 사용
  - 교차확인(cross-validation) 방법
    - k번 반복 측정한 결과를 평균낸 값을 최종값으로 한다

#### 지도학습
- **[1] 분류 분석 (Classification Analysis)** 
  - 로지스틱 회귀분석 (Logistic Regression)
  - 의사결정나무 (Decision Tree) 
- **[2] 앙상블 분석 (Ensemble Analysis)** 
  - 배깅 (Bagging)
  - 부스팅 (Boosting)
  - 랜덤 포레스트 (Random Forest)
- **[3] 인공신경망 분석 (Artificial Neural Network (ANN) Analysis)** 

#### 비지도학습
- **[4] 군집 분석 (Cluster Analysis)** 
  - 계층적 군집 (Hierarchical Clustering)
  - 분할적 군집 (K-Means Clustering)
  - 혼합 분포 군집 (Mixture Distribution Clustering)
  - SOM (Self Organizing Map)
- **[5] 연관 분석 (Association Analysis)** 
  - 장바구니분석 (Market Basket Analysis)
  - 순차패턴 (Sequence Analysis)
  
-------------

#### [1] 분류 분석 (Classification Analysis)

- **로지스틱 회귀분석 (Logistic Regression)**
  - 반응변수가 범주형인 경우 사용
  - 하향식 기법
  - 설명변수가 한 개인 경우, (B1 > 0 이면 S자 모양, B1 < 0 이면 역 S자 모양)
  - 오즈비 (odds ratio)
    - 오즈: 성공확률 / 실패확률
    - 오즈비: 오즈1 / 오즈2

- **의사결정나무 (Decision Tree)**
  - 입력값에 대하여 출력값을 예측하는 분류 모형
  - 하향식 의사결정 흐름이 핵심 공통 개념
  - 뿌리 마디에서 아래로 내려갈수록 각 마디에서의 불순도는 점차 감소한다
  - 과정
    - 성장(growing)
    - 가지치기(pruning)
      - 마디 내 자료 수가 일정 수(가령 5) 이하 일때 분할 정지
      - 비용-복잡도 가지치기 (cost-complexity pruning) 이용
      - 가장 어려운 부분
    - 타당성 평가
    - 해석 및 예측
  - 장점
    - 설명이 용이
    - 계산이 복잡 X
    - 잡음에 민감 X
    - 불필요 변수에 큰 영향 X
    - 설명변수와 목표변수에 수치형 및 범주형 모두 사용 가능
    - 정확도가 높음
  - 단점
    - 과대적합 발생 가능
    - 분류 경계선 부근 자료에 대한 오차가 큼
    - 설명변수 간 중요도 판단 힘듦
  - 불순도의 척도
    - 목표변수가 범주형일 경우 의사결정나무의 분류규칙 선택에 활용됨
    - 카이제곱 통계량
    - 지니 지수 (클수록 이질적이며 순수도가 낮음)
    - 엔트로피 지수 (클수록 순수도가 낮음)
  - 의사결정나무 알고리즘
    - Classification and Regression Tree (CART)
    - C4.5와 C.5.0
    - CHi-squared Automatic Interaction Detection (CHAID)

#### [2] 앙상블 분석 (Ensemble Analysis)

- 여러 개의 예측모형을 만단 후, 조합하여 하나의 최종 예측 모형을 만듬 

- **배깅 (Bagging)**
  - Bootstraping 자료 및 예측모형 생성 후, voting을 통하여 결합 후 최종 모델 생성
- **부스팅 (Boosting)**
  - 예측력이 약한 모형들을 결합하여 강한 예측모형 생성
- **랜덤 포레스트 (Random Forest)**
  - 많은 무작위성을 주어, 약한 학습기들을 생성한 후, 선형결합하여 최종 학습기 생성
  - 분류 분석 문제를 해결하기 위한 의사결정나무와 같은 방법론이지만, 의사결정나무에서의 과대적합/과대적합 문제를 해결
  - Bootstrap에서 샘플에 한번도 선택되지 않는 원데이터는 전체 샘플의 약 36.8%
  - 장점
    - 정확도, 예측력 높음 (입력변수가 많을 경우)
    - 과대적합/과대적합 문제를 해결
  - 단점
    - 이론적 설명과 최종 결과 해석이 어려움

#### [3] 인공신경망 분석 (Artificial Neural Network (ANN) Analysis)

- 가중치를 반복적으로 조정하며 학습
- 역전파(Back propagation) 알고리즘 등을 이용
- 지도학습(Supervised Learning) 이다
- 뉴런의 활성화 함수
  - 시그모이드 함수
    - 로지스틱 회귀분석과 유사
    - 0 ~ 1 확률을 가짐
  - Softmax 함수
    - 표준화 함수
    - 목표치가 다범주인 경우에서 각 범주에 속할 사후확률 제공
  - Relu 함수
    - 입력값 0 이하는 0, 0 이상은 x값
- 입력 자료의 선택에 매우 민감
- 일반적으로 초기값은 0 근처로 랜덤하게 선택
- 학습모드
  - 온라인 학습 (online learning): 관측값 순차적 투입
  - 확률적 학습 (probabilistic learning): 관측값 랜덤 투입
  - 배치 학습 (batch learning): 관측값 동시 투입
- 은닉층(hidden layer)
  - 가능하면 하나로 설정 (자동 설정 X)
- 은닉노드(hidden node)
  - 적절히 큰 값으로 놓고 가중치를 감소 시키며 적용 (자동 설정 X)

#### [4] 군집 분석 (Cluster Analysis)

- 군집에 속한 객체들의 유사성과 서로 다른 군집에 속한 객체간의 상이성을 규명
- 군집의 개수나 구조에 대한 가정 없이, 데이터 사이의 거리를 기준으로 군잡화 유도
- 거리 계산법 (연속형)
  - 유클리디안 (Euclidean) 거리
  - 표준화 (Statistical) 거리
  - 마할라노비스 (Mahalanobis) 거리
    - 변수의 표준화와 변수 간의 상관성을 동시에 고려
  - 체비셰프 (Chebychev) 거리
  - 맨하탄 (Manhattan) 거리
  - 캔버리 (Canbberra) 거리
  - 민코우스키 (Minkowski) 거리
- 거리 계산법 (범주형)
  - 자카드 거리
  - 자카드 계수
  - 코사인 거리
  - 코사인 유사도
- **계층적 군집 (Hierarchical Clustering)**
  - n 개의 군집으로 시작해 개수를 점점 줄여나가는 방법
  - 최단 연결법 (single linkage, nearest neighbor)
  - 최장 연결법 (complete linkage, farthest neighbor)
  - 평균 연결법 (average linkage)
  - 와드 연결법 (ward linkage)
    - 편차들의 제곱합 고려
    - 정보의 손실 최소화
  - 군집화
    - 덴드로그램 사용
      - 계층적 군집 분석 결과를 나태내는 도표
      - 적절한 군집 수 선정 (보통 5개 이하)
- **분할적 군집 (Partitional Clustering)**
  - n 개의 개체를 g 개의 군집으로 나눌 수 있는 모든 가능한 방법 점검 후, 최적화 군집 형성
  - k-means clustering
    - 주어진 데이터를 k 클러스터로 묶는 알고리즘 (사용자가 k 값 지정)
    - 각 클러스터와 거리 차이의 분산을 최소화
    - 과정
      - 원하는 군집 개수(k)와 초기값(seed) 정하고 seed 중심으로 군집 형성
      - 각 데이터를 가장 가까운 seed가 있는 군집으로 분류
      - 각 군집의 seed 값 다시 계산
      - *모든 계체가 군집으로 할당 될때까지* 위 과정 반복
    - 특징
      - 초기 중심값 선정에 따라 결과가 달라질 수 있다
    - 장점
      - 알고리즘이 단순, 빠르게 수행, 분석 방법 적용 용이
      - 많은양 데이터 다루기 가능 (계층적 분석에 비해)
      - 사전정보가 없어도 의미있는 자료구조 찾기 가능
      - 다양한 형태의 데이터에 적용
    - 단점
      - 군집의 수, 가중치와 거리 정의 어려움
      - 결과 해석이 어려움 (사전 목적이 주어지지 않음으로)
      - 볼록한 형태             (convex)가 아닐 경우, 성능이 떨어짐
      - 초기 군집수(k) 결정 어려움
      - 이상치 자료에 민감하다
      
- **혼합 분포 군집 (Mixture Distribution Clustering)**
  - k개의 각 모형은 군집을 의미하며, 각 데이터는 추정된 k개의 모형 중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집의 분류가 이루어짐
  - 흔히 혼합모형에서의 모수와 가중치의 추정(최대가능도추정)에는 EM 알고리즘이 사용된다
  - EM 알고리즘의 진행 과정
    - [1] E-step: 임의의 파라미터 값을 정함 -> 잠재변수 Z의 기대치 계산
    - [2] M-step: 잠재변수 Z의 기대치를 이용하여 파라미터를 추정 -> Likelihood가 최대화인가 확인
    - [3] 파라미터 추정값 도출
  - 혼합 분포 군집 모형의 특징
    - k-means clustering 의 절차와 유사하지만, 확률분포를 도입하여 군집을 수행한다
    - k-means clustering 과 같이 이상치 자료에 민감하므로 사전에 조치가 필요하다

- **Self Organizing Map (SOM)**
  - SOM은 비지도 신경망으로 고차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 정렬 하여 지도의 형태로 형상화한다
  - 이러한 형상화는 입력 변수의 위치 관계를 그대로 보존한다는 특징이 있다
  - 다시 말해 실제 공간의 입력 변수가 가까이 있으면, 지도상에도 가까운 위치에 있게 된다
  - 비지도학습 (Unsupervised Learning) 이다
  - 구성
    - 입력층(input layer)
      - 입력 변수의 개수와 동일하게 뉴런 수가 존재한다
    - 경쟁층(competitive layer)
      - SOM은 경쟁 학습으로 각각의 뉴런이 입력 벡터와 얼마나 가까운가를 계산하여 연결 강도(connection weight)를 반복적으로 재조정하여 학습한다
      - 이 과정을 거치면서 연결강도는 입력 패턴과 가장 유사한 경쟁층 뉴런이 승자가 된다
  - 특징
    - 역전파(Back propagation) 알고리즘 등을 이용하는 인공신경망과 달리 단 하나의 전방 패스(feed-forward flow)를 사용함으로써 속도가 매우 빠르다. 따라서, 실시간 학습처리를 할 수 있는 모형이다

#### [5] 연관 분석 (Association Analysis)

- 흔히 장바구니분석(Market Basket Analysis) 또는 서열분석(Sequence Analysis) 이라고 불린다
- 사건들 간의 규칙을 발견하기 위해 적용한다
- 연관규칙의 척도
  - 지지도(Support)
    - 전체 거래 중 항목 A와 항목 B를 동시에 포함하는 거래의 비율로 정의한다
  - 신뢰도(Confidence)
    - 항목 A를 포함한 거래 중에서 항목 A와 항목 B가 같이 포함될 확률이다
  - 향상도(Lift)
    - A 가 구매되지 않았을 때의 품목 B 구매확률에 비해 A가 구매됬을 때의 품목 B의 증가 비이다
    - 연관규칙 A -> B는 품목 A와 품목 B의 구매가 서로 관련이 없는 경우에 향상도가 1이 된다
- 절차
  - 최소 지지도 설정 -> 품목 중 최소 지지도를 넘는 품목 분류 -> 2가지 품목 집합 생성 -> 반복적으로 수행해 빈발품목 집합을 찾음
- 장점
  - 탐색적인 기법으로 조건 반응으로, 결과를 쉽게 이해할 수 있다
  - 분석 방향이나 목적이 특별히 없는 경우 목적변수가 없으므로 유용하게 활용 된다
  - 데이터를 변환 없이 그 자체로 이용할 수 있는 간단한 자료 구조를 갖는다
  - 분석을 위한 계산이 간단하다
- 단점
  - 품목수가 증가하면 분석에 필요한 계산은 기하급수적으로 늘어난다
  - 너무 세분화한 품목을 갖고 연관성 규칙을 찾으면 의미없는 분석이 될 수도 있다
  - 거래량이 적은 품목은 당연히 포함된 거래수가 적을 것이고, 규칙 발견 시 제외하기가 쉽다
- 순차패턴(Sequence Analysis)
  - 연관성분석에 시간이라는 개념을 포함시켜 순차적으로 구매 가능성이 큰 상품군을 찾아내는 것이다
- 기존 연관성 분석의 이슈
  - 대용량 데이터에 대한 연관성분석이 불가능하다
- 최근 연관성 분석 동향
  - Apriori 알고리즘
    - 모든 품목집합에 대한 지지도를 전부 계산하는 것이 아니라, 최소 지지도 이상의 빈발항목집합을 찾은 후 그것들에 대해서만 연관규칙을 계산하는 것이다
    - 지지도가 낮은 후보 집합 생성 시 아이템의 개수가 많아지면 계산 복잡도가 증가한다는 문제점을 가지고 있다
  - FP-Growth 알고리즘
    - 후보 빈발항목집합을 생성하지 않고, FP-Tree(Frequent Pattern Tree)를 만든 후 분할정복 방식을 통해 Apriori 알고리즘 보다 더 빠르게 빈발항목집합을 추출할 수 있는 방법이다
    - Apriori 알고리즘의 약점을 보완 하기 위해 고안된 것으로 데이터베이스를 스캔하는 횟수가 적고, 빠른 속도로 분석이 가능하다
    