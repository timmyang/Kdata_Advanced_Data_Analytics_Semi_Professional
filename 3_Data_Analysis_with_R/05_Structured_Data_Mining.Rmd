---
title: "Part 5: Structured Data Mining"
output: rmarkdown::github_document
---

# Table of Contents

- **Chapter 1 - 데이터 마이닝의 개요 (Data Mining Outline)**
  - 데이터 마이닝
  - 데이터 마이닝의 분석 방법
  - 분석 목적에 따른 작업 유형과 기법
  - 데이터 마이닝 추진단계
  - 데이터 마이닝을 위한 데이터 분할
  - 성과 분석
  
- **Chapter 2 - 분류분석 (Classification Analysis)**
  - 분류분석과 예측분석
  - 로지스틱 회귀분석 (Logistic Regression)
  - 의사결정나무
  - 불순도의 여러 가지 측도
  - 의사결정나무 알고리즘
  - 의사결정나무 예시
  
- **Chapter 3 - 앙상블 분석 (Ensemble Analysis)**
  - 앙상블 (Ensemble)

- **Chapter 4 - 인공신경망 분석 (Artificial Neural Network Analysis)**
  - 인공신경만 분석 (ANN)

- **Chapter 5 - 군집분석 (Cluster Analysis)**
  - 군집분석
  - 거리
  - 계층적 군집분석
  - 비계층적 군집분석
  - 혼합 분포 군집 (mixture distribution clustering)
  - SOM (Self Organizing Map)
  - 최신 군집분석 기법들

- **Chapter 6 - 연관분석 (Association Analysis)**
  - 연관규칙
  - 기존 연관성 분석의 이슈
  - 최근 연관성 분석 동향
  - 연관성 분석 활용 방안
  - 연관성 분석 예제
  
  
# Chapter 1 - 데이터 마이닝의 개요 (Data Mining Outline)

## 1. 데이터 마이닝

#### 가. 개요

- 데이터 마이닝은 대용량 데이터에서 의미있는 패턴을 파악하거나 예측하여 의사결정에 활용하는 방법이다

#### 나. 통계분석 과의 차이점

- 통계분석은 가설이나 가정에 따른 분석이나 검증을 하지만 **데이터 마이닝** 은 다양한 수리 알고리즘을 이용해 데이터 베이스의 **데이터로부터 의미있는 정보를 찾아내는 방법을 통칭** 한다

#### 다. 종류

- **정보를 찾는 방법론에 따른 종류**
  - 인공지능 (Artificial Intelligence)
  - 의사결정나무 (Decision Tree)
  - K-평균군집화 (K-means clustering)
  - 연관분석 (Association Analysis)
  - 회귀분석 (Regression)
  - 로짓분석 (Logit Analysis)
  - 최근접이웃 (Nearest Neighborhood)
 
- **분석대상, 활용목적, 표현방법에 따른 분류**
  - 시각화분석 (Visualization Analysis)
  - 분류 (Classification)
  - 군집화 (Clustering)
  - 포케스팅 (Forecasting)
  
#### 라. 사용분야

- 병원에서 환자 데이터를 이용해서 해당 환자에게 발생 가능성이 높은 병을 예측
- 기존 환자가 응급실에 왔을 때 어떤 조치를 먼저 해야 하는지를 결정
- 고객 데이터를 이용해 해당 고객의 우량/불량을 예측해 대출적격 여부 판단
- 세관 검사에서 입국자의 이력과 데이터를 이용해 관세물품 반입 여부를 예측

#### 마. 데이터 마이닝의 최근 환경

- 데이터 마이닝 도구가 다양하고 체계화되어 환경에 적합한 제품을 선택하여 활용 가능하다
- 알고리즘에 대한 깊은 이해가 없어도 분석에 큰 어려움이 없다
- 분석 결과의 품질은 분석가의 경험과 역량에 따라 차이가 나기 때문에 분석 과제의 복잡성이나 중요도가 높으면 풍부한 경험을 가진 전문가에게 의뢰할 필요가 있다
- 국내에서 데이터 마이닝이 적용된 시기는 1990년대 중반이다
- 2000년대에 비즈니스 관점에서 데이터 마이닝이 CRM의 중요한 요소로 부각되었다
- 대중화를 위해 많은 시도가 있었으나, 통계학 전문가와 대기업 위주로 진행되었다

**기출문제**  
**94. 다음 중 기업이 보유하고 있는 거래 데이터, 고객 데이터 등과 외부 데이터를 포함하는 모든 데이터를 기반으로 새로운 규칙 등을 발견하고 이를 비즈니스 의사결정 등에 정보로 활용하고자 하는 일련의 작업을 무엇이라고 하는가?**

1. 회귀분석  
2. **데이터 마이닝**  
3. 데이터 웨어하우징  
4. 의사결정지원시스템


## 2. 데이터 마이닝의 분석 방법

- **지도학습(Supervised Data Prediction)**
  - 의사결정나무
  - 인공신경망
  - 일반화 선형 모형
  - 회귀분석
  - 로지스틱 회귀분석
  - 사례기반 추론
  - 최근접 이웃법

- **비지도학습(Unsupervised Data Prediction)**
  - OLAP (Online Analytical Processing)
  - 연관성 규칙발견 (Association Rule Discovery, Market Basket)
  - 군집분석 (K-means Clustering)
  - SOM (Self Organizing Map)
  
**기출문제**  
**95. 정형데이터 마이닝 중에서 비지도학습 기법을 사용하여 분석을 수행하였다. 가장 적절한 것은?**  
(a) **고객의 거래 구매 패턴을 분석하여 고객이 구매하지 않은 상품을 추천**  
(b) 우편물에 인쇄된 우편번호 판별 분석을 통해 우편물을 자동으로 분류  
(c) 동일 차종의 수리 보고서 데이터를 분석하여 차량 수리에 소요되는 시간을 예측  
(d) **상품을 구매할 때 그와 유사한 상품을 구매한 고객들의 구매 데이터를 분석하여 제시**

## 3. 분석 목적에 따른 작업 유형과 기법

- **예측 (Predictive Modeling)**
  - **분류 규칙 (Classification)**
    - 가장 많이 사용되는 작업으로 과거의 데이터부로부터 고객특성을 찾아내어 분류모형을 만들어 이를 토대로 새로운 레코드의 결과값을 예측하는 것으로 목표 마케팅 및 고객 신용평가 모형에 활용됨
    - 회귀분석, 판별분석, 신경망, 의사결정나무
    
- **설명 (Descriptive Modeling)**
  - **연관 규칙 (Association Analysis)**
    - 데이터 안에 존재하는 항목간의 종속관계를 찾아내는 작업으로, 제품이나 서비스의 교차판매(Cross Selling), 매장진열(Display), 첨부우편(Attached Mailings), 사기적발(Fraud Detection) 등의 다양한 분야에 활용됨
    - <사용기법> 동시발생 매트릭스
  - **연속 규칙 (Sequence)**
    - 연관 규칙에 시간관련 정보가 포함된 형태로, 고객의 구매이력(History) 속성이 반드시 필요하며, 목표 마케팅(Target Marketing)이나 일대일 마케팅(One-to-One Marketing)에 활용됨
    - <사용기법> 동시발생 매트릭스
  - **데이터 군집화 (Clustering)**
    - 고객 레코드들을 유사한 특성을 지닌 몇 개의 소그룹으로 분할하는 작업으로 작접의 특성이 분류규칙(Classification)과 유사하나 분석대상 데이터에 결과값이 없으며, 판촉활동이나 이벤트 대상을 선정하는데 활용됨
    - <사용기법> K-Means Clustering

## 4. 데이터 마이닝 추진단계

#### 가. 1단계: 목적 설정

- 데이터 마이닝을 통해 무엇을 왜 하는지 명확한 목적(이해관계자 모두 동의하고 이해할 수 있는)을 설정한다
- 전문가가 참여해 목적에 따라 사용할 모델과 필요한 데이터를 정의한다

#### 나. 2단계: 데이터 준비

- 고객정보, 거래정보, 상품 마스터정보, 웹로그 데이터, 소셜 네트워크 데이터 등 다양한 데이터를 활용한다
- IT 부서와 사전에 협의하고 일정을 조율하여 데이터 접근 부하에 유의하여야 하며, 필요시 다른 서버에 저장하여 운영에 지장이 없도록 데이터를 준비한다
- 데이터 정제를 통해 데이터의 품질을 보장하고, 필요시 데이터를 보강하여 충분한 양의 데이터를 확보한다

#### 다. 3단계: 가공

- 모델링 목적에 따라 목적 변수를 정의한다
- 필요한 데이터를 데이터 마이닝 스프트웨어에 적용할 수 있는 형식으로 가공한다

#### 라. 4단계: 기법 적용

- 1단계에서 명확한 목적에 맞게 데이터 마이닝 기법을 적용하여 정보를 추출한다

#### 마. 5단계: 검증

- 데이터 마이닝으로 추출된 정보를 검증한다
- 테스트 데이터와 과거 데이터를 활용하여 최적의 모델을 선정한다
- 검증이 완료되면 IT 부서와 협의해 상시 데이터 마이닝 결과를 업무에 적용하고 보고서를 작성항여 추가수익과 투자대비성과(ROI) 등으로 기대효과를 전파한다


## 5. 데이터 마이닝을 위한 데이터 분할

#### 가. 개요

- 모델 평가용 테스트 데이터와 구축용 데이터로 분할하여, 구축용 데이터로 모형을 생성하고 테스트 데이터로 모형이 얼마나 적합한 지를 판단한다

#### 나. 데이터 분할

- **1) 구축용(training data, 50%)**
  - 추정용, 훈련용 데이터라고도 불리며 데이터 마이닝 모델을 만드는데 활용한다
  
- **2) 검정용(validation data, 30%)**
  - 구축된 모형의 과대추정 또는 과소추청을 미세 조정하는 것에 활용한다
  
- **3) 시험용(test data, 20%)**
  - 테스트 데이터나 과거 데이터를 활용하여 모델의 성능을 검증하는데 활용한다
  
- **4) 데이터의 양이 충분하지 않거나 입력 변수에 대한 설명이 충분한 경우**
  - 가) 홀드아웃(hold-out) 방법
    - 주어진 데이터를 랜덤하게 두 개의 데이터로 구분하여 사용하는 방법으로 주로 학습용(training data)과 시험용(test data)로 분리하여 사용한다
  - 나) 교차확인(cross-validation) 방법
    - 주어진 데이터를 k개의 하부집단으로 구분하여, k - 1개의 집단을 학습용으로 나머지는 하부집단으로 검증용으로 설정하여 학습한다
    - k번 반복 측정한 결과를 평균낸 값을 최종값으로 한다
    - 주로 10-fold 교차분석을 많이 사용한다
    

## 6. 성과분석

- **Condition Positive + Prediction Positive**
  - True Positive (TP)
- **Condition Positive + Prediction Negative**
  - False Negative (FN)
- **Condition Negative + Prediction Negative**
  - True Negative (TN)
- **Condition Negative + Prediction Positive**
  - False Positive (FP)
 
#### 가. 오분류에 대한 추정치

- **[1] 정분류율(Accuracy)**
  - Accuracy = (TP + TN) / (TP + TN + FP + FN)

- **[2] 오분류율(Error Rate)**
  - Error Rate = (FP + FN) / (TP + TN + FP + FN)
  
- **[3] 특이도(Specificity) = TNR: True Negative Rate**
  - Specificity = TN / (TN + FP)
  
- **[4] 민감도(Sensitivity) = 재현율(Recall) = TPR: True Positive Rate**
  - Sensitivity = TP / (TP + FN)
  
- **[5] 정확도(Precision)**
  - Precision = TP / (TP + FP)
  
- **[6] F1 Score**
  - F1 = 2 x (Precision x Recall) / (Precision + Recall)
  
  
#### 나. ROCR 패키지로 성과분석

- **1) ROC Curve (Receiver Operating Characteristic Curve)**
  - ROC Curve란 가로축을 FPR(False Positive Rate = 1 - TNR(specificity)) 값으로 두고, 세로축을 TPR(Sensitivity) 값으로 두어 시각화한 그래프이다
  - 2진 분류(binary classification)에서 모형의 성능을 평가하기 위해 많이 사용되는 척도이다
  - 그래프가 왼쪽 상단에 가깝게 그려질수록 올바르게 예측한 비율은 높고, 잘못 예측한 비율은 낮음을 의미한다
  - 따라서 **ROC 곡선 아래의 면적을 의미하는 AUROC(Area Under ROC)** 값이 크면 클수록(1에 가까울수록) 모형의 성능이 좋다고 평가한다
  - TPR(True Positive Rate = Sensitivity)
    - 1인 케이스에 대한 1로 예측한 비율
  - FPR(False Positive Rate = 1 - Specificity)
    - 0인 케이스에 대한 1로 잘못 예측한 비율
  - AUROC(Area Under ROC)를 이용한 정확도의 판단 기준
    - **excellent (A)**
      - 0.9 - 1.0
    - **good**
      - 0.8 - 0.9
    - **fair**
      - 0.7 - 0.8
    - **poor**
      - 0.6 - 0.7
    - **fail**
      - 0.5 - 0.6

- **2) ROC Curve와 AUROC의 활용 예시**
  - AUROC = (AR + 1) / 2
  
- **3) R 실습 코드**
  - ROCR 패키지는 binary classification 만 지원가능
  
```{r, message = FALSE, warning = FALSE}
library(rpart)
# install.packages("party")
library(party)
# install.packages("ROCR")
library(ROCR)

x <- kyphosis[sample(1:nrow(kyphosis), nrow(kyphosis), replace = F), ]

x.train <- kyphosis[1:floor(nrow(x) * 0.75), ]
x.evaluate <- kyphosis[floor(nrow(x) * 0.75):nrow(x), ]

x.model <- cforest(Kyphosis ~ Age + Number + Start, data = x.train)

x.evaluate$prediction <- predict(x.model, newdata = x.evaluate)
x.evaluate$correct <- x.evaluate$prediction == x.evaluate$Kyphosis

print(paste("% of predicted classification correct", mean(x.evaluate$correct)))

x.evaluate$probabilities <- 1 - unlist(treeresponse(x.model, newdata = x.evaluate), use.names = F)[seq(1, nrow(x.evaluate) * 2, 2)]
```

  - 그래프 1

```{r}
pred <- prediction(x.evaluate$probabilities, x.evaluate$Kyphosis)
perf <- performance(pred, "tpr", "fpr")
plot(perf, main = "ROC curve", colorize = T)
```

  - 그래프 2
  
```{r}
perf <- performance(pred, "lift", "rpp")
plot(perf, main = "lift curve", colorize = T)
```

#### 다. 이익도표(Lift chart)

- **1) 이익도표의 개념**
  - 이익도표는 분류모형의 성능을 평가하기 위한 척도로, 분류된 관측치에 대해 얼마나 예측이 잘 이루어졌는지를 나타내기 위해 임의로 나눈 각 등급별로 반응검출율, 반응률, 리프트 등의 정보를 산출하여 나타내는 도표이다
  - 2000명의 전체고객 중 381명이 상품을 구매한 경우에 대해 이익도표를 만드는 과정을 예로 들어보면, 먼저 데이터셋의 각 관측치에 대한 예측확률을 내림차순으로 정렬한다
  - 이후 데이터를 10개의 구간으로 나눈 다음 각 구간의 반응율(% response)을 산출한다
  - 또한 기본 향상도(baseline lift)에 비해 반응률이 몇 배나 높은지를 계산하는데 이것을 향상도(Lift)라고 한다
  - 이익도표의 각 등급은 예측확률에 따라 매겨진 순위이기 떄문에, 상위 등급에서는 더 높은 반응률을 보이는 것이 좋은 모형이라고 평가할 수 있다
  
- **2) 이익도표의 활용 예시**
  - 전체 2000명 중 381명이 구매
  - Frequency of "buy"
    - 2000명 중 실제로 구매한 사람
  - % Captured Reponse
    - 반응검출율 = 해당 등급의 실제 구매자 / 전체 구매자
  - % Response
    - 반응률 = 해당 등급의 실제 구매자 / 200명
  - Lift
    - 향상도 = 반응률 / 기본 향상도
    - 좋은 모델이라면 Lift가 빠른 속도로 감소해야 한다
  - 등급별로 향상도가 급격하게 변동할 수록 좋은 모형이라고 할 수 있고, 각 등급별로 향상도가 들쭉날쭉하면 좋은 모형이라고 볼 수 없다
  
#### 참고

- **과적합/과대적합 (Overfitting)**
  - 모형이 학습용 데이터(training data)를 과하게 학습하여, 학습 데이터에 대해서는 높은 정확도를 나타내지만 테스트 데이터 혹은 다른 데이터에 적용할 때는 성능이 떨어지는 현상을 의미한다

- **과소적합 (Underfitting)**
  - 모형이 너무 단순하여 데이터 속에 내제되어 있는 패턴이나 규칙을 제대로 학습하지 못하는 경우를 의히만다
  

**기출문제**  
**1. 오분류표를 사용한 평가 지표 중 아래 설명이 나타내는 지표는 무엇인가?**  
정확도(precision)와 재현율(recall)은 한 지표의 값이 높아지면 다른 지표의 값이 낮아질 가능성이 높은 관계를 지니고 있어 이러한 효과를 보정하여 하나의 지표로 만들어 낸 지표

1. **F1**  
2. 민감도  
3. 특이도  
4. 오즈비  

**97-1. 분류모형을 평가할 때 사용되는 지표로 x축에는 (1 - 특이도(specificity))를 나타내며, y축에는 민감도를 나타내어, 특이도가 어떤 관계를 갖고 변하는지를 이차원 평면상에 표현한 곡선은 무엇인가?**

ROC Curve



# Chapter 2 - 분류분석 (Classification Analysis)

## 1. 분류분석과 예측분석

#### 가. 분류분석의 정의

- 데이터가 어떤 그룹에 속하는지 예측하는데 사용되는 기법이다
- 클러스터링과 유사하지만, 분류분석은 각 그룹이 정의되어 있다
- 교사학습(supervised learning)에 해당하는 예측기법이다

#### 나. 예측분석의 정의

- 시계열 분석 처럼 시간에 따른 값 두 개만을 이용해 앞으로의 매출 또는 온도 등을 예측하는 것
- 모델링을 하는 입력 데이터가 어떤 것인지에 따라 특성이 다르다
- 여러 개의 다양한 설명변수(독립변수)가 아닌, 한 개의 설명변수로 생각하면 된다

#### 다. 분류분석, 예측분석의 공통점과 차이점

- **공통점**
  - 레코드의 특정 속성의 값을 미리 알아맞히는 점이다

- **차이점**
  - 가) 분류
    - 레코드(튜플)의 **범주형 속성** 의 값을 알아맞히는 것이다
  - 나) 예측
    - 레코드(튜플)의 **연속형 속성** 의 값을 알아맞히는 것이다

#### 라. 분류, 예측의 예

- **분류**
  - 가) 학생들의 국어, 영어, 수학 점수를 통해 내신등급을 알아맞히는 것
  - 나) 카드회사에서 회원들의 가입 정보를 통해 1년 후 신용등급을 알아맞히는 것
  
- **예측**
  - 가) 학생들의 여러 가지 정보를 입력하여 수능점수를 알아맞히는 것
  - 나) 카드회사 회원들의 가입정보를 통해 연 매출액을 알아맞히는 것

#### 마. 분류 모델링

- 신용평가모형(우량, 불량)
- 사기방지모형(사기, 정상)
- 이탈모형(이탈, 유지)
- 고객세분화(VVIP, VIP, GOLD, SILVER, BRONZE)

#### 바. 분류 기법

- 회귀분석, 로지스틱 회귀분석(Logistic Regression)
- **의사결정나무(Decision Tree)**, CART(Classification and Regression Tree), C5.0
- 베이지안 분류(Bayesian Classification), Naive Bayesian
- **인공신경망(ANN, Artificial Neural Network)**
- 지지도 벡터기계(SVM, Support Vector Machine)
- k 최근접 이웃(KNN, K-Nearest Neighborhood)
- 규칙기반의 분류와 사례기반추론(Case-Based Reasoning)

**기출문제**  
**98. 한 보험회사에서는 자사 고객의 보험갱신 여부를 고객의 연구통계학적 특성, 보험가입 채널, 상품 종류 등의 정보를 사용하여 예측하려고 한다. 다음 중 가장 적절한 분석 기법은 무엇인가?**

1. 시계열 분석  
2. **랜덤포레스트**  
3. k-means 군집분석  
4. 주성분분석


## 2. 로지스틱 회귀분석 (Logistic Regression)

- 반응변수가 범주형인 경우에 적용되는 회귀분석모형이다
- 새로운 설명변수(또는 예측변수)가 주어질 때 반응변수의 각 범주(또는 집단)에 속할 확률이 얼아민지를 추정(예측모형)하여, 추정 확률을 기준치에 따라 분류하는 목적(분류모형)으로 활용된다
- 이때 모형의 적합을 통해 추정된 확률을 사후확률(Posterior Probability)이라고 한다  
  log[P(y)/(1 - P(y))] - alpha + B1 x1 + ... + Bk xk  
  P(y) = P(y = 1 | x), x = (x1, ..., xk)  

- exp(B1)의 의미는 나머지 변수(x1, ..., xk)가 주어질 때, x1이 한 단위 증가할 때마다 성공(Y = 1)의 오즈가 몇배 증가하는지를 타나내는 값이다  
  P(y) = [exp(alpha + B1 x1 + ... + Bk xk)] / [1 + exp(alpha + B1 x1+ ... + Bk xk)]  
       = 1 / [1 + exp[-(alpha + B1 x1 + ... + Bk xk)]]  

- 위 식은 다중로지스틱 회귀모형이며, 그래프잉 형태는 설명변수가 한 개(x1)인 경우 해당 회귀 계수 B1의 부호에 따라 S자 모양(B1 > 0) 또는 역 S자 모양 (B1 < 0)을 가진다  
- 표준 로지스틱 분포의 누적분포함수(c.d.f)를 F(x)라 할 때  
   P(y) = F(alpha + B1 x1 + ... + Bk xk)    
   위 식과 동일한 표현이며, 표준 로지스틱 분포의 누적분포함수로 성공의 확률을 추정한다
- <참고>
- **오즈비(odds ratio)**
  - 오즈(odds)는 성공할 확률이 실패할 확률의 몇배인지를 타나내는 확률이다
  - 오즈비(odds ratio)는 오즈의 비율이다
  - ex)
    - 성공확률1: 0.8
    - 실패확률1: 0.2
      - 오즈: 0.8/0.2 = 4
    - 성공확률2: 0.1
    - 실패확률2: 0.9
      - 오즈: 0.1/0.9 = 1/9
    - 오즈비
      - 4/(1/9) = 36
- 선형회귀분석과 로지스틱 회귀분석의 비교
  - **선형회귀분석**
    - 종속변수
      - 연속형 변수
    - 계수 추정법
      - 최소제곱법
    - 모형 검정
      - F 검정, T 검정
  - **로지스틱 회귀분석**
    - 종속변수
      - (0, 1)
    - 계수 추정법
      - 최대 우도 추정법
    - 모형 검정
      - 카이제곱 검정(X^2 test)

- <참고>
- **최대우도 추정법(MLE: Maximum Likelihood Estimation)**
  - 모수가 미지의 theta인 확률분포에서 뽑은 표본(관측치) x들을 바탕으로 theta를 추정하는 기법
  - 우도(likelihood)는 이미 주어진 표본 x들에 비추어봤을 때 모집단의 모수 theta에 대한 추정이 그럴듯한 정도를 말한다
  - 우도 L(theta | x)는 theta가 전제되었을 때 표본 x가 등장할 확률인 p(x | theta)에 비례한다

- glm() 함수를 활용하여 로지스틱 회귀분석을 실행한다
- R 코드
  - glm(종속변수 ~ 독립변수1 + ... + 독립변수k, family = binomial, data = 데이터셋명)


## 3. 의사결정나무

#### 가. 정의

- 의사결정나무는 분류함수를 의사결정 규칙으로 이뤄진 **나무 모양으로 그리는 방법** 이다
- 나무구조는 연속적으로 발생하는 의사결정 문제를 **시각화** 해 의사결정이 이뤄지는 시점과 성과를 한눈에 볼 수 있게 한다
- 계산결과가 의사결정나무에 직접 나타나기 떄문에 해석이 간편하다
- 의사결정나무는 주어진 **입력값에 대하여 출력값을 예측하는 모형** 으로 분류나무와 회귀나무 모형이 있다
- <참고>
- 뿌리마디(root node)
  - 시작되는 마디로 전체 자료를 포함
- 자식마디(child node)
  - 하나의 마디로부터 분리되어 나간 2개 이상의 마디들
- 부모마디(parent node)
  - 주어진 마디의 상위 마디
- 끝마디(terminal node)
  - 자식마디가 없는 마디
- 중간마디(internal node)
  - 부모마디와 자식마디가 모두 있는 마디
- 가지(branch)
  - 뿌리마디로부터 끝마디까지 연결된 마디들
- 깊이(depth)
  - 뿌리마디부터 끝마디까지의 중간 마디들의 수


#### 나. 예측력과 해석력

- 기대 집단의 사람들 중 가장 많은 반응을 보일 **고객의 유치방안을 예측** 하고자 하는 경우에는 **예측력에 치중** 한다
- 신용평가에서는 심사 결과 부적격 판정이 나온 경우 고객에게 부적격 **이유를 설명** 해야하므로 **해석력에 치중** 한다


#### 다. 의사결정나무의 활용

- **[1] 세분화**
  - 데이터를 비슷한 특성을 갖는 몇 개의 그룹으로 분할해 그룹별 특성을 발견하는 것이다
- **[2] 분류**
  - 여러 예측변수들에 근거해 관측개체의 목표변수 범주를 몇 개의 등급으로 분류하고자 하는 경우에 사용하는 기법이다
- **[3] 예측**
  - 자료에서 규칙을 찾아내고 이를 이용해 미래의 사건을 예측하고자 하는 경우이다
- **[4] 차원축소 및 변수선택**
  - 매우 많은 수의 예측변수 중에서 목표변수에 큰 영향을 미치는 변수들을 골라내고자 하는 경우에 사용하는 기법이다
- **[5] 교호작용(interaction)효과의 파악**
  - 여러 개의 예측변수들을 결합해 목표변수에 작용하는 규칙을 파악하고자 하는 경우
  - 범주의 병합 또는 연속형 변수의 이산화
    = 범주형 목표변수의 범주를 소수의 몇 개로 병합하거나 연속형 목표변수를 몇 개의 등급으로 이산화 하고자 하는 경우이다


#### 라. 의사결정나무의 특징

- **<장점>**
  - 결과를 누구에게나 설명하기 용이하다
  - 모형을 만드는 방법이 계산적으로 복잡하지 않다
  - 대용량 데이터에서도 빠르게 만들 수 있다
  - 비정상 잡음 데이터에 대해서도 민감함이 없이 분류할 수 있다
  - 한 변수와 상관성이 높은 다른 불필요한 변수가 있어도 크게 영향을 받지 않는다
  - 설명변수나 목표변수에 수치형변수와 범주형변수를 모두 사용 가능하다
  - 모형 분류 정확도가 높다

- **<단점>**
  - 새로운 자료에 대한 과대적합이 발생할 가능성이 높다
  - 분류 경계선 부근의 자료값에 대해서 오차가 크다
  - 설명변수 간의 중요도를 판단하기 쉽지 않다


#### 마. 의사결정나무의 분석 과정

- 의사결정나무의 형성과정은 크게 성장(growing), 가지치기(pruning), 타당성 평가, 해석 및 예측으로 이루어진다

- **[1] 성장 단계**
  - 각 마디에서 적절한 최적의 분리규칙(splitting rule)을 찾아서 나무를 성장시키는 과정으로 적절한 정지규칙(stopping rule)을 만족하면 중단한다
- **[2] 가지치기 단계**
  - 오차를 크게 할 위험이 높거나 부적절한 추론규칙을 가지고 있는 가지 또는 불필요한 가지를 제거하는 단계이다
- **[3] 타당성 평가 단계**
  - 이익도표(gain chart), 위험도표(risk chart), 혹은 시험자료를 이용하여 의사결정나무를 평가하는 단계이다
- **[4] 해석 및 예측 단계**
  - 구축된 나무모형을 해석하고 예측모형을 설정한 후 예측에 적용하는 단계이다
  

#### 바. 나무의 성장

- 훈련자료(xi, yi), i = 1, 2, ..., n 으로 나타내자. 여기서 xi = (xi1, ..., xip) 이다
- 나무모형의 성장과정은 x들로 이루어진 입력 공간을 재귀적(recurrent)으로 분할하는 과정이다  
  <img src="https://render.githubusercontent.com/render/math?math=$R_1(j, A) = x_j \in A$">  
  <img src="https://render.githubusercontent.com/render/math?math=$R_2(j, A^c) = x_j \in A^c$">

- **[1] 분리 규칙(splitting rule)**
  - 분리 변수(split variable)가 연속형인 경우
    - A = xj <= s
  - 분리변수가 범주형 {1, 2, 3, 4} 인 경우
    - A = 1, 2, 4 와 A^c = 3 으로 나눌 수 있다
  - 최적 분할의 결정은 불순도 감소량을 가장 크게 하는 분할이다  
    <img src="https://render.githubusercontent.com/render/math?math=$\Delta i(t) = i(t) - p_L i(t_L) - p_R i(t_R), i(t) = \Sigma_{i} (y_i - \overline{y}_{t})^2$">
  - 각 단계에서 최적 분리기준에 의한 분할을 찾은 다음 각 분할에 대하여도 동일한 과정을 반복한다

- **[2] 분리 기준(splitting critertion)**
  - 이산형 목표변수
    - **카이제곱 통계량 p 값**
      - p 값이 가장 작은 예측변수와 그 때의 최적분리에 의해서 자식마디를 생성
    - **지니 지수**
      - 지니 지수를 감소시켜주는 예측변수와 그 때의 최적분리에 의해서 자식마디를 선택
    - **엔트로피 지수**
      - 엔트로피 지수가 가장 작은 예측 변수와 이 때의 최적분리에 의해 자식마디를 형성
  - 연속형 목표변수
    - **분산분석에서 F 통계량**
      - p 값이 가장 작은 예측변수와 그 때의 최적분리에 의해서 자식마디를 형성
    - **분산의 감소량**
      - 분산의 감소량을 최대화 하는 기준의 최적분리에 의해서 자식마디를 형성
      
- **[3] 정지 규칙(stopping rule)**
  - 더 이상 분리가 일어나지 않고, 현재의 마디가 끝마디가 되도록 하는 규칙이다
  - 정지기준(stopping criterion)
    - 의사결정나무의 깊이(depth)를 지정, 끝마디의 레코드 수의 최소 개수를 지정한다
    

#### 사. 나무의 가지치기(Pruning)
  - 너무 큰 나무모형은 자료를 과대적합하고 너무 작은 나무모형은 과소적합할 위험이 있다
  - 나무의 크기를 모형의 복잡도로 볼 수 있으며 최적의 나무 크기는 자료로부터 추정하게 된다
  - 일반적으로 사용되는 방법은 마디에 속하는 **자료가 일정 수 (가령 5)이하** 일 때 분할을 정지하고 **비용-복잡도 가지치기(cost complexity pruning)를 이용** 하여 성장시킨 나무를 가지치기하게 된다

**기출문제**  
**100. 다음 중 의사결정나무의 특성으로 가장 부적절한 것은?**  

1. 의사결정나무 무형의 결과는 누구에게나 설명이 용이하다  
2. 의사결정나무 알고리즘의 모형 정확도는 다른 분류모형에 뒤지지 않는다  
3. 의사결정나무를 만드는 방법은 계산적으로 복잡하지 않다  
4. **의사결정나무 알고리즘은 정상적인 데이터에 대해서만 민감함이 없이 분류할 수 있다**


## 4. 불순도의 여러 가지 측도

- 목표변수가 범주형 변수인 의사결정나무의 분류규칙을 선택하기 위해서는 카이제곱 통계량, 지니 지수, 엔트로피 지수를 활용한다
- **[1] 카이제곱 통계량**
  - 카이제곱 통계량은 각 셀에 대한 (실제도수 - 기대도수)^2 / 기대도수 의 합으로 구할 수 있다
  - 기대도수 = 열의합계 x 합의합계 / 전체합계  
    <img src="https://render.githubusercontent.com/render/math?math=$\chi^2 = \Sigma_{i = 1}^{k} (O_i - B_i)^2 / B_i$"> (k: 범주의 수, Oi = 실제도수, Bi = 기대도수)
    
- **[2] 지니 지수**
  - 노드의 불순도를 나타내는 값이다
  - 지니지수의 값이 클수록 이질적(Diverse)이며 순수도(Purity)가 낮다고 볼 수 있다  
    <img src="https://render.githubusercontent.com/render/math?math=$Gini(T) = 1 - \Sigma_{l = 1}^{k} p_l ^2$">

- **[3] 엔트로피 지수**
  - 열역학에서 쓰는 개념으로 무질서 정도에 대한 측도이다
  - 엔트로피 지수의 값이 클수록 순수도(Purity)가 낮다고 볼 수 있다
  - 엔트로피 지수가 가장 작은 예측 변수와 이때의 최적분리 규칙에 의해 자식마디를 형성하다  
    <img src="https://render.githubusercontent.com/render/math?math=$Entropy(T) = -(\Sigma_{l = 1}^{k} p_l \ log_2 \ p_l)$">
    

## 5. 의사결정나무 알고리즘

#### 가. CART(Classification and Regression Tree)

- 앞에서 설명한 방식의 가장 많이 활용되는 의사결정나무 알고리즘으로 불순도의 측도로 출력(목적) 변수가 범주형일 경우 지니수를 이용, 연속형인 경우 분산을 이용한 이진분리(binary split)를 사용한다
- 개별 입력변수 뿐만 아니라 입력변수들의 선형결합들 중에서 최적의 분리를 찾을 수 있다

#### 나. C4.5와 C5.0

- CART와는 다르게 각 마디에서 다지분리(multiple split)가 가능하며 범주형 입력변수에 대헤서는 범주의 수만큼 분리가 일어난다
- 불순도의 측도로는 엔트로피 지수를 사용한다

#### 다. CHAID(Chi-squared Automatic Interaction Detection)

- 가지치기를 하지 않고 적당한 크기에서 나무모형의 성장을 중지하며 입력변수가 반드시 범주형 변수이어야 한다
- 불순도의 축도로는 카이제곱 통계량을 사용한다


## 6. 의사결정나무 예시

#### 가. party 패키지를 이용한 의사결정 나무

- party 패키지는 의사결정나무를 사용하기 편한 다양한 분류 패키지 중 하나이다
- 분실값을 잘 처리하지 못하는 문제를 갖고 있는 것이 단점이다
- tree 에 투입된 데이터가 표시가 되지 않거나 predict 가 실패하는 경우 문제가 발생할 수 있다
- **[1] iris data 를 이용한 분석**
  - iris data 의 70% 는 training data, 30% 는 test data 로 생성한다

```{r}
idx <- sample(2, nrow(iris), replace = TRUE, prob = c(0.7, 0.3))

train.data <- iris[idx == 2, ]
test.data <- iris[idx == 1, ]
```

- **[2] train.data 를 이용하여 모형생성**

```{r, message = FALSE}
# install.packages("party")
library(party)
# library(sandwich)

iris.tree <- ctree(Species ~ ., data = train.data)

plot(iris.tree)
plot(iris.tree, type = "simple")
```

- **[3] 예측된 데이터와 실제 데이터의 비교**

```{r}
table(predict(iris.tree), train.data$Species)
```

- **[4] test data 를 적용하여 정확성 확인**

```{r}
test.pre <- predict(iris.tree, newdata = test.data)
table(test.pre, test.data$Species)
```


# Chapter 3 - 앙상블 분석 (Ensemble Analysis)

## 1. 앙상블 (Ensemble)

#### 가. 정의

- 주어진 자료로부터 여러 개의 예측모형들을 만든 후 예측모형들을 조합하여 하나의 최종 예측 모형을 만드는 방법으로 다중 모델 조합(combining multiple models), 분류기 조합(classifier combination)이 있다  
  New Data -> Training Samples -> Learning Algorithms -> Classifier 1, 2, 3 -> Combined Classifiers -> Prediction
  
#### 나. 학습방법의 불안정성

- 학습자료의 작은 변화에 의해 예측모형이 크게 변하는 경우, 그 학습방법은 불안정하다
- 가장 안정적인 방법으로는 k-nearest neighbor(가장 가까운 자료만 변하지 않으면 예측 모형이 변하지 않음), 선형회귀무형(최소제곱법으로 추정해 모형 결정)이 존재한다
- 가장 불안정한 방법으로는 의사결정나무가 있다

#### 다. 앙상블 기법의 종류

- **[1] 배깅**
  - Breiman(1994)에 의해 제안된 배깅은 주어진 자료에서 여러 개의 붓스트랩(bootstrap) 자료를 생성하고 각 붓스트랩 자료에 예측모형을 만든 후 결합하여 최종 예측모형을 만드는 방법이다. 붓스트랩(bootstrap)은 주어진 자료에서 동일한 크기의 표본을 랜던 복원추출로 뽑은 자료를 의미한다
  - 보팅(voting)은 여러 개의 모형으로부터 산출된 결과를 다수결에 의해서 최종 결과를 선정하는 과정이다
  - 최적의 의사결정나무를 구축할 때 가장 어려운 부분이 가지치기(puring)이지만 배깅에서는 가지치기를 하지 않고 최대로 성장한 의사결정나무들을 활용한다
  - 훈련자료의 모집단의 분포를 모르기 때문에 실제 문제에서는 평균예측모형을 구할 수 없다. 배깅은 이러한 문제를 해결하기 위해 훈련자료를 모집단으로 생각하고 평균예측모형을 구하여 분산을 줄이고 예측력을 향상시킬 수 있다.
  
- **[2] 부스팅**
  - 예측력이 약한 모형(waek learner)들을 결합하여 강한 예측모형을 만드는 방법이다
  - 부스팅 방법 중 Freund & Schapire 가 제안안 **Adaboost** 는 이진분류 문제에서 랜덤 분류기 보다 조금 더 좋은 분류기 n 개에 각각 가중치를 설정하고 n 개의 분류기를 결합하여 최종 분류기를 만드는 방법을 제안하였다. (단, 가중치의 합은 1)
  - 훈련오차를 빨리 그리고 쉽게 줄일 수 있다
  - 배깅에 비해 많은 경우 예측오차가 향상되어 Adaboost의 성능이 배깅보다 뛰어난 경우가 많다

- **[3] 랜덤 포레스트(random forest)**
  - Breiman(2001)에 의해 개발된 랜덤 포레스트는 의사결정나무의 특징인 분산이 크다는 점을 고려하여 배깅과 부스팅보다 더 많은 무작위성을 주어 **약한 학습기들을 생성한 후 이를 선형 결합하여 최종 학습기를 만드는 방법** 이다
  - randomForest 패키지는 random input 에 따른 forest of tree 를 이용한 분류방법이다
  - 랜덤한 forest 에는 많은 트리들이 생성된다
  - 수천 개의 변수를 통해 변수제거 없이 실행되므로 정확도 측면에서 좋은 성과를 보인다
  - 이론적 설명이나 최종 결과에 대한 해석이 어렵다는 단점이 있지만 예측력이 매우 높은 것으로 알려져 있다. 특히 입력변수가 많은 경우, 배깅과 부스팅과 비슷하거나 좋은 예측력을 보인다
  - **<참고>**
    - 붓스트랩(bootstrap)은 주어진 자료에서 단순랜덤 복원추출 방법을 활용하여 동일한 크기의 표본을 여러개 생성하는 샘플링 방법이다. 붓스트랩을 통해 100개의 샘플을 추출하더라도 **샘플에 한번도 선택되지 않은 원데이터** 가 발생할 수 있는데 **전체 샘플의 약 36.8%** 가 이에 해당한다
    
  - 가) randomForest 패키지를 이용한 분석(iris data)
    - 모형 만들기
```{r}
# install.packages("randomForest")
library(randomForest)

idx <- sample(2, nrow(iris), replace = TRUE, prob = c(0.7, 0.3))

train.data <- iris[idx == 2, ]
test.data <- iris[idx == 1, ]

r.f <- randomForest(Species ~., data = train.data, ntree = 100, proximity = TRUE)
```
    - 오차율 계산하기
```{r}
table(predict(r.f), train.data$Species)
```
    - 그래프 그리기 1
```{r}
plot(r.f)
```
    - 그래프 그리기 2
```{r}
varImpPlot(r.f)
```
    - test data 예측
```{r}
pre.rf <- predict(r.f, newdata = test.data)
table(pre.rf, test.data$Species)
```
    - 그래프 그리기 3
```{r}
plot(margin(r.f, test.data$Species))
```
    
**기출문제**  
**1. 앙상블 모형(Ensemble) 이란 주어진 자료로부터 여러 개의 예측모형을 만든 후 이러한 예측모형들을 결합하여 하나의 최종 예측모형을 만드는 방법을 말한다. 다음 중 앙상블모형에 대한 설명으로 적절하지 않은 것은?**

1. 배깅은 주어진 자료에서 여러 개의 부스트랩(Bootstrap) 자료를 생성하고 각 부스트랩 자료에 예측모형을 만든 후 결합하여 최종 모형을 만드는 방법이다  
2. **부스팅은 배깅의 과정과 유사하여 재표본 과정에서 각 자료에 동일한 확률을 부여하여 여러 모형을 만들어 결합하는 방법이다**  
3. 랜덤 포레스트(Random Forest)는 의사결정나무모형의 특징인 분산이 크다은 점을 고려하여 배깅보다 더 많은 무작위성을 추가한 방법으로 약한 학습기들을 생성하고 이를 선형 결합해 최종 학습기를 만드는 방법이다  
4. 앙상블모형은 훈련을 한 뒤 예측을 하는데 사용하므로 교사학습(Supervised Learning)이다


# Chapter 4 - 인공신경망 분석 (Artificial Neural Network Analysis)

## 1. 인공신경망 분석 (ANN)

#### 가. 인공신경망이란?

- 인간 뇌를 기반으로 한 추론 모델이다
- 뉴런은 기본적인 정보처리 단위이다

#### 나. 인공신경망의 연구

- 1943년 매컬럭(McCulloch)과 피츠(Pitts)
  - 인간의 뇌를 수많은 신경세포가 연결된 하나의 디지털 네트워크 모형으로 간주하고 신경세포의 신호처리 과정을 모형화하여 단순 패턴분류 모형을 개발했다
- 헵(Hebb)
  - 신경세포(뉴런) 사이의 연결강도(weight)를 조정하여 학습규칙을 개발했다
- 로젠블럿(Rosenblatt, 1955)
  - 퍼셉트론(Perceptron)이라는 인공세포를 개발했다
  - 비선형성의 한계점 발생 -XOR(Exclusive OR) 문제를 풀지 못하는 한계를 발견하였다
- 홉필드(Hopfild), 러멜하트(Rumelhart), 맥클랜드(McClelland)
  - **역전파 알고리즘(Backpropagation)** 을 활용하여 비선형성을 극복한 다계층 퍼셉트론으로 새로운 인공신경망 모형이 등장했다
  
#### 다. 인간의 뇌를 형상화한 인공신경망

- **[1] 인간 뇌의 특징**
  - 100억개의 뉴런과 6조 개의 시냅스의 결합체이다
  - 인간의 뇌는 현존하는 어떤 컴퓨터보다 빠르고 매우 복잡하고, 비선형적이며, 병렬적인 벙보 처리 시스템과 같다
  - 적응성에 따라 "잘못된 답"에 대한 뉴런들 사이의 연결은 약화되고, "올바른 답"에 대한 연결이 강화된다

- **[2] 인간의 뇌 모델링**
  - 뉴런은 가중치가 있는 링크들로 연결되어 있다
  - 뉴런은 여러 입력 신호를 받지만 출력 신호는 오직 하나만 생성한다

#### 라. 인공 신경망의 학습

- 신경망은 **가중치를 반복적으로 조정하며 학습** 한다
- 뉴런은 링크(link)로 연결되어 잇고, 각 링크에는 수치적인 가중치가 있다
- 인공 신경망은 신경망의 가중치를 초기화하고 훈련 데이터를 통해 가중치를 갱신하여 신경망의 구조를 선택하고, 활용할 학습 알고리즘을 결정한 후 신경망을 훈련시킨다

#### 마. 인공신경망의 특징

- **[1] 구조**
  - 입력 링크에서 여러 신호를 받아서 새로운 활성화 수준을 계산하고, 출력 링크로 출력 신호를 보낸다
  - 입력신호는 미가공 데이터 또는 다른 뉴런의 출력이 될 수 있다
  - 출력신호는 문제의 최종적인 해(solution)가 되거나 다른 뉴런에 입력 될 수 있다

- **[2] 뉴런의 계산**
  - 뉴런은 전이함수(transfer function), 즉 활성화 함수(activation function)를 사용한다
  - 활성화 함수를 이용해 출력을 결정하며 입력신호의 가중치 합을 계산하여 임계(threshold)값과 비교한다
  - 가중치 합이 임계값 보다 작으면 뉴런의 출력은 -1, 같거나 크면 +1 을 출력한다
  - <img src="https://render.githubusercontent.com/render/math?math=$X = \Sigma_{i = 1}^{n} \chi_i w_i$">  
  - Y = + 1 (if X >= theta)  
    Y = - 1 (if X <  theta)
  - X 는 뉴런으로 들어가는 입력의 순 가중합  
    x_i 는 입력 i 의 값  
    w_i 는 입력 i 의 가중치  
    n 은 뉴런의 입력 개수  
    Y 는 뉴런의 출력
    
- **[3] 뉴런의 활성화 함수**
  - **시그모이드 함수** 의 경우 로지스틱 회귀분석과 유사하며, 0 ~ 1 의 확률값을 가진다  
    <img src="https://render.githubusercontent.com/render/math?math=$Y^{sigmoid} = 1 / (1 + e^{-X})$">
  - softmax 함수
    - 표준화 지수 함수로도 불리며, 출력값이 여러개로 주어지고 목표치가 다범주인 경우 각 범주에 속할 사후확률을 제공하는 함수이다  
      <img src="https://render.githubusercontent.com/render/math?math=$y_i = exp(z_j) / \Sigma_{i = 1}^{L} exp(z_i), \ j = 1, ..., L$">
  - Relu 함수
    - 입력값이 0 이하는 0, 0 이상의 값은 x 값을 가지는 함수이며, 최근 딥러닝에서 많이 활용하는 활성화 함수이다

- **[4] 단일 뉴런의 학습(단층 퍼셉트론)**
  - 임의값(x) -> w -> 선형결합(\Sigma) -> 하드 리미터 (theta, 임계값) -> 출력값(Y)
  - 퍼셉트론은 선형 결합기와 하드 리미터로 구성된다
  - 초평면(hyperplane)은 n 차원 공간을 두개의 영역으로 나눈다
  - 초평면을 선형 분리 함수로 정의한다  
    <img src="https://render.githubusercontent.com/render/math?math=$\Sigma_{i = 1}^{n} x_i w_i - \theta = 0$">
    
#### 바. 신경망 모형 구축시 고려사항

- **[1] 입력 변수**
  - 신경망 모형은 그 복잡성으로 인하여 입력 자료의 선택에 매우 민감하다
  - 입력변수가 범주형 또는 연속형 변수일 때 아래의 조건이 산경망 모형에 적합하다
    - 범주형 변수
      - 모든 범주에서 일정 빈도 이상의 값을 갖고 각 범주의 빈도가 일정할 때
    - 연속형 변수
      - 입력변수 값들의 범위가 변수간의 큰 차이가 없을 때
  - 연속형 변수의 경우 그 분포가 평균을 중심으로 대칭이 아니면 좋지 않은 결과를 도출하기 때문에 아래와 같은 방법을 활용한다
    - 변환
      - 고객의 소득(대부분 평균 미만이고 특정 고객의 소득이 매우 큰): 로그 변환
    - 범주화
      - 각 범주의 빈도가 비슷하게 되도록 설정

- **[2] 가중치의 초기값과 다중 최소값 문제**
  - 역전파 알고리즘은 초기값에 따라 결과가 많이 달라지므로 초기값의 선택은 매우 중요한 문제이다
  - 가중치가 0 이면 시그모이드 함수는 선형이 되고 신경망 모형은 근사적으로 선형모형이 된다
  - 일반적으로 초기값은 0 근처로 랜덤하게 선택하므로 초기 모형은 선형모형에 가깝고, 가중치 값이 증가할수록 비선형모형이 된다. (초기값이 0 이면 반복하여도 값이 전혀 변하지 않고, 너무 크면 좋지 않은 해를 주는 문제점을 내포하고 있어 주의 필요)
  
- **[3] 학습모드**
  - **가) 온라인 학습 모드 (online learning mode)**
    - 각 관측값을 순차적으로 하나씩 신경망에 투입하여 가중치 추정값이 매번 바뀐다
    - 일반적으로 속도가 빠르며, 특히 훈련자료에 유사값이 많은 경우 그 차이가 더 두드러진다
    - 훈련자료가 비정상성(non-stationary)과 같이 특이한 성질을 가진 경우가 좋다
    - 국소 최솟값에서 벗어나기가 더 쉽다
  - **나) 확률적 학습 모드 (probabilistic learning mode)**
    - 온라인 학습 모드와 같으나 신경망에 투입되는 관측값의 순서가 랜덤하다
  - **다) 배치 학습 모드 (batch learning mode)**
    - 전체 훈련자료를 동시에 신경망에 투입한다
  - <참고> **학습률**
    - 학습률은 처음에은 큰 값으로 정하고 반복 수행과정을 통해 해에 가까울수록 학습률이 0 에 수렴한다

- **[4] 은닉층(hidden layer)과 은닉노드(hidden node)의 수**
  - 신경망을 적용할 때 가장 중요한 부분이 모형의 선택이다. (은닉층의 수와 은닉노드의 수 결정)
  - 은닉층과 은닉노드가 적으면 과소적합 문제가 발생한다
  - 은닉층의 수가 하나인 신경망은 범용 근사자(universal approximator) 이므로 모든 매끄러운 함수를 근사적으로 표현할 수 있다. 그러므로 가능하면 은닉층은 하나로 선정한다
  - 은닉노드의 수는 적절히 큰 값으로 놓고 가중치를 감소(weight decay) 시키며 적용하는 것이 좋다
  
- **[5] 과대 적합 문제**
  - 신경망에서는 많은 가중치를 추정해야 하므로 과대적합 문제가 빈번하다
  - 알고리즘의 조기종료와 가중치 감소 기법으로 해결할 수 있다
  - 모형이 적합(fit)하는 과정에서 검증오차가 증가하기 시작하면 반복을 중지하는 조기종료를 시행한다
  - 선형모형의 능형회귀(ridge regression)와 유사한 가중치 감소라는 벌점화 기법을 활용한다
  - <참고>
    - 딥러닝(Deep Learing)
      - 머신러닝의 한 분야로서 인공신경망의 한계를 극복하기 위해 제안된 심화신경망(Deep Neural Network)를 활용한 방법이다
    - 딥러닝 소프트웨어
      - 딥러닝 구동을 위한 SW에는 Tensorflow, caffe, Theano, MXnet 등이 있다
      - 딥러닝은 최근 음성과 이미지인식, 자연어처리, 헬스케어 등의 전반적인 분야에 활용되고 있다

**기출문제**  
**101. 신경망 모형은 자신이 가진 데이터로부터 반복적인 학습과정을 거쳐 패턴을 찾아내고 이를 일반화하는 예측방법이다. 다음 중 신경망 모형에 대한 설명으로 부적절한 것은 무엇인가?**

1. 피드포워드 신경망은 정보가 전방으로 전달되는 것으로 생물학적 신경계에서 나타나는 형태이다  
2. **은닉층의 뉴런 수 와 개수는 신경망 모형에서 자동으로 설정된다. 뉴런 수가 많으면 예측력이 좋아지고, 뉴런 수가 적으면 입력 데이터를 충분히 표현하지 못할 수 있다**  
3. 일반적으로 인공신경망은 다층퍼셉트론을 의미한다. 다층퍼셋트론에서 정보의 흐름은 입력층에서 시작하여 은닉층을 거쳐 출력층으로 진행된다  
4. 역전파 알고리즘은 연결강도를 갱산하기 위해 예측된 결과와 실제 값의 차이인 에러의 역전파를 통해 가중치를 구하는 데서 시작되었다  

**102. 신경망 모형에서 아래의 식으로 계산되는 함수로서 표준화 지수 함수로 불리며, 출력값 z가 여러 개로 주어지고, 목표치가 다범주인 경우 각 범주에 속할 사후 확률을 제공하여 출력 노드에 주로 사용되는 함수는?**  

<img src="https://render.githubusercontent.com/render/math?math=$y_i = exp(z_j) / \Sigma_{i = 1}^{L} exp(z_i)$">

softmax 함수: 표준화지수 함수로도 불리며, 출력값이 여러개로 주어지고 목표치가 다범주인 경우 각 범주에 속할 사후확률을 제공하는 함수이다


# Chapter 5 - 군집분석 (Cluster Analysis)

## 1. 군집분석

#### 가. 개요

- 각 객체(대상)의 유사성을 측정하여 유사성이 높은 대상 집단을 분류하고, 군집에 속한 객체들의 유사성과 서로 다른 군집에 속한 객체간의 상이성을 규명하는 분석방법이다
- 특성에 따라 고객을 여러 개의 배타적인 집단으로 나누는 것이다
- 결과는 구체적인 군집분석 방법에 따라 차이가 나타날 수 있다
- 군집의 개수나 구조에 대한 가정 없이 데이터들 사이의 거리를 기준으로 군집화를 유도한다
- 마케팅 조사에서 소비자들의 상품구매행동이나 life style 에 따른 소비자군을 분류하여 시장 전략 수립 등에 활용한다
- 계층적 군집(Hierarchical Clustering) vs. 분할적 군집(Partitional Clustering)

#### 나. 특징

- **[1] 요인분석과의 차이점**
  - 요인분석은 유사한 변수를 함께 묶어주는 것이 목적이다
  
- **[2] 판별분석과의 차이점**
  - 판별분석은 사전에 집단이 나누어져 있는 자료를 통해 새로운 데이터를 기존의 집단에 할당하는 것이 목적이다


## 2. 거리

군집분석에서는 관측 데이터 간 유사성이나 접근성을 측정해 어느 군집으로 묶을 수 있는지 판단해야 한다

#### 가. 연속형 변수의 경우

- 유클리디안 거리(Euclidean distance)
  - 데이터간의 유사성을 측정할 때 많이 사용하는 거리
  - 통계적 개념이 내포되어 있지 않아 변수들의 산포 정도가 전혀 감안되어 있지 않다  
    <img src="https://render.githubusercontent.com/render/math?math=$d(x, y) = \sqrt{(x_1 - y_1)^2 + ... +(x_p - y_p)^2} = \sqrt{(x - y)(x - y)}$">
    
- 표준화 거리(Statistical distance)
  - 해당변수의 표준편차로 척도 변환한 후 유클리디안 거리를 계산하는 방법이다
  - 표준화하게 되면 척도의 차이, 분산의 차이로 인한 왜곡을 피할 수 있다  
    <img src="https://render.githubusercontent.com/render/math?math=$d(x, y) = \sqrt{(x - y)D^{-1}(x - y)}, D = diag{s_11, ..., s_{pp}}$">
    
- 마할라노비스(Mahalanobis) 거리
  - 통계적 개념이 포함된 거리이며 변수들의 산포를 고려하여 이를 표준화한 거리(standardized distance)이다
  - 두 벡터 사이의 거리를 산포를 의미하는 표본공분산으로 나눠주어야 하며, 그룹에 대한 사전 지식 없이는 표본공분산S를 계산할 수 없으므로 사용하기 곤란하다  
    <img src="https://render.githubusercontent.com/render/math?math=$d(x, y) = \sqrt{(x - y)S^{-1}(x - y)}, S = \{ S_{ij} \}$"> 는 공분산 행렬
    
- 체비셰프(Chebychev) 거리
  - <img src="https://render.githubusercontent.com/render/math?math=$d(x, y) = max_i |x_i - y_i|$">
  
- 맨하탄(Manhattan) 거리
  - 유클리디안 거리와 함께 가장 많이 사용되는 거리로 맨하탄 도시에서 건물에서 건물을 가기 위한 최단 거리를 구하기 위해 고안된 거리이다  
    <img src="https://render.githubusercontent.com/render/math?math=$d(x, y) = \Sigma_{i = 1}^{p} |x_i - y_i|$">
    
- 캔버라(Canbberra) 거리
  - <img src="https://render.githubusercontent.com/render/math?math=$d(x, y) = \Sigma_{i = 1}^{p} |x_i - y_i|/(x_i + y_i)$">
  
- 민코우스키(Minkowski) 거리
  - 맨하탄 거리와 유클리디안 거리를 한번에 표현한 공식으로 L1 거리(맨하탄 거리), L2 거리(유클리디안 거리)라 불리고 있다  
  <img src="https://render.githubusercontent.com/render/math?math=$d(x, y) = (\Sigma_{i = 1}^{p} |x_i - y_i|^m)^{1/m}, m = 1, m = 2$">
  
#### 나. 범주형 변수의 경우

- 자카드 거리  
  <img src="https://render.githubusercontent.com/render/math?math=$1 - J(A, B) = (|A \cup B | - |A \cap B |) / |A \cup B |$">
- 자카드 계수  
  <img src="https://render.githubusercontent.com/render/math?math=$J(A, B) = |A \cap B| / |A \cup B|$">
- 코사인 거리
  - 문서를 유사도를 기준으로 분류 혹은 그룹핑 할 때 유용하게 사용한다  
    <img src="https://render.githubusercontent.com/render/math?math=$d_{cos}(A, B) = 1 - (A \cdot B)/(||A||_2 \cdot ||B||_2)$">
- 코사인 유사도
  - 두 개체의 백터 내적의 코사인 값을 이용하여 측정된 벡터간의 유사한 정도이다
  - 두 벡터 A, B에 대해 코사인 유사도는 아래와 같이 정의된다  
    <img src="https://render.githubusercontent.com/render/math?math=$cosine \: \: similarity = (A \cdot B)/(||A||_2 \cdot ||B||_2)$">

**기출문제**  
**103. 계층적 군집방법은 두 개체(또는 군집) 간의 거리(또는 비유사성)에 기반하여 군집을 형성해 나가므로 거리에 대한 정의가 필요한데, 다음 중 변수의 표준화와 변수간의 상관성을 동시에 고려한 통계적 거리로 적절한 것은?**

1. 표준화 거리  
2. 민코우스키 거리  
3. **마할라노비스 거리**  
4. 자카드 계수  


## 3. 계층적 군집분석

- 계층적 군집 방법은 n개의 군집으로 시작해 점차 군집의 개수를 줄여 나가는 방법이다
- 계층적 군집을 형성하는 방법에는 합병형 방법(agglomerative: bottom-up)과 분리형 방법(Divisive: top-down)이 있다

#### 가. 최단 연결법 (single linkage, nearest neighbor)

- n * n 거리행렬에서 거리가 가장 가까운 데이터를 묶어서 군집을 형성한다
- 군집과 군집 또는 데이터와의 거리를 계산 시 최단거리(min)를 거리로 계산하여 거리행렬 수정을 진행한다
- 수정된 거리행렬에서 거리가 가까운 데이터 또는 군집을 새로운 군집으로 형성한다

#### 나. 최장 연결법 (complete linkage, farthest neighbor)

- 군집과 군집 또는 데이터와의 거리를 계산할 때 최장거리(max)를 거리로 계산하여 거리행렬을 수정하는 방법이다

#### 다. 평균 연결법 (average linkage)

- 군집과 군집 또는 데이터와의 거리를 계산할 때 평균(mean)을 거리로 계산하여 거리행렬을 수정하는 방법이다

#### 라. 와드 연결법 (ward linkage)

- 군집 내 편차들의 제곱합을 고려한 방법이다
- 군집 간 정보의 손실을 최소화하기 위해 군집화를 진행한다

#### 마. 군집화

- 거리행렬을 통해 가장 가까운 거리의 객체들간의 관계를 규명하고 덴드로그램을 그린다
- 덴드로그램을 보고 군집의 개수를 변화해 가면서 적절한 군집 수를 선정한다
- 군집의 수는 분석 목적에 따라 선정할 수 있지만 대부분 5개 이상의 군집은 잘 활용하지 않는다
- 군집화 단계
  - [1] 거리행렬을 기준으로 덴드로그램을 그린다
  - [2] 덴드로그램의 최상단부터 세로축의 개수에 따라 가로선을 그어 군집의 개수를 선택한다
  - [3] 각 객체들의 구성을 고려해서 적절한 군집수를 선정한다

## 4. 비계층적 군집분석

n개의 개체를 g개의 군집으로 나눌 수 있는 모든 가능한 방법을 점검해 최적화한 군집을 형성하는 것이다

#### 가. k-평균 군집분석(k-means clustering)의 개념

- 주어진 데이터를 k개의 클러스터로 묶는 알고리즘으로, 각 클러스터와 거리 차이의 분산을 최소화하는 방식으로 동작한다

#### 나. k-평균 군집분석(k-means clustering) 과정

- 원하는 군집의 개수와 초기 값(seed)들을 정해 seed 중심으로 군집을 형성한다
- 각 데이터를 거리가 가장 가까운 seed가 있는 군집으로 분류한다
- 각 군집의 seed 값을 다시 계산한다
- 모든 개체가 군집으로 할당될 때까지 위 과정들을 반복한다

#### 다. k-평균 군집분석의 특징

- 거리 계산을 통해 군집화가 이루어지므로 **연속형 변수에 활용이 가능** 하다
- k개의 **초기 중심값은 임의로 선택이 가능** 하며 가급적이면 멀리 떨어지는 것이 바람직하다
- 초기 중심값을 임의로 선택할 때 일렬(위아래, 좌우)로 선택하면은 군집에 혼합되지 않고 층으로 나누어질 수 있어 주의하여야 한다. **초기 중심값의 선정에 따라 결과가 달라** 질 수 있다
- 초기 중심으로부터의 오차 제곱합을 최소화하는 방향으로 군집이 형성되는 **탐욕적(greedy) 알고리즘** 이므로 안정된 군집은 보장하나 최적이라는 보장은 없다
- **장점**
  - 알고리즘이 단순하며, 빠르게 수행되어 분석 방법 적용이 용이하다
  - 계층적 군집분석에 비해 많은 양의 데이터를 다룰 수 있다
  - 내부 구조에 대한 사전정보가 없어도 의미있는 자료구조를 찾을 수 있다
  - 다양한 형태의 데이터에 적용이 가능하다
- **단점**
  - 군집의 수, 가중치와 거리 정의가 어렵다
  - 사전에 주어진 목적이 없으므로 결과 해석이 어렵다
  - 잡음이나 이상값의 영향을 많이 받는다
  - 볼록한 형태가 아닌 (non-convex) 군집이 (예를 들어 U형태의 군집) 존재할 경우에는 성능이 떨어진다
  - 초기 군집수 결정에 어려움이 있다

**기출문제**  
**104. 다음 중 k-평균법을 수행하는 적절한 절차는?**  
가. 원하는 군집의 개수와 초기 값(seed)을 정해 seed 중심으로 군집을 형성한다  
나. 각 군집의 seed 값을 다시 계산한다  
다. 각 데이터를 거리가 가장 가까운 seed가 있는 군집으로 분류한다  
라. 모든 개체가 군집으로 할당될 때까지 위 과정을 반복한다  

1. 가 -> 나 -> 다 -> 라  
2. 다 -> 가 -> 나 -> 라  
3. **가 -> 다 -> 나 -> 라**  
4. 다 -> 나 -> 가 -> 라


## 5. 혼합 분포 군집 (mixture distribution clustering)

#### 가. 개요

- 모형 기반(model-based)의 군집 방법이며 데이터가 k개의 모수적 모형(흔히 정규분포 또는 다변량 정규분포를 가정함)의 가중합으로 표현되는 모집단 모형으로부터 나왔다는 가정하에서 모수와 함께 가중치를 자료로부터 추정하는 방법을 사용한다
- k개의 각 모형은 군집을 의미하며, 각 데이터는 추정된 k개의 모형 중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집의 분류가 이루어진다
- 흔히 혼합모형에서의 모수와 가중치의 추정(최대가능도추정)에는 **EM 알고리즘** 이 사용된다

#### 나. 혼합 분포모형으로 설명할 수 있는 데이터의 형태

- (a)는 자료의 분포형태가 다봉형의 형태를 띠므로 단일 분포로의 적합은 적절하지 않으며, 대략 3개 정도의 정규분포 결합을 통해 설명될 수 있을 것으로 생각할 수 있다
- (b)의 경우에도 여러 개의 이변량 정규분포의 결합을 통해 설명될 수 있을 것이다. 두 경우 모두 반드시 정규분포로 제한할 필요는 없다

#### 다. EM (Expectation-Maximization) 알고리즘의 진행 과정

- 각 자료에 대해 Z의 조건부분포(어느 집단에 속할 지에 대한)로부터 조건부 기댓값을 구할 수 있다
- 관측변수 X와 잠재변수 Z를 포함하는 (X, Z)에 대한 로그-가능도함수(이를 보정된(augmented) 로그-가능도함수라 함)에 Z 대신 상수값인 Z의 조건부 기댓값을 대입하면, 로그-가능도함수를 최대로 하는 모수를 쉽게 찾을 수 있다. (M-단계) 갱신된 모수 추정치에 대해 위 과정을 반복한다면 수렴하는 값을 얻게 되고, 이는 최대 가능도 추정치로 사용될 수 있다
- E-단계: 잠재변수 Z의 기대치 계산
- M-단계: 잠재변수 Z의 기대치를 이용하여 파라미터를 추정
  - [1] E-step(임의의 파라미터 값을 정함 -> Z 기대치 계산)
  - [2] M-step(Z의 기대치를 이용하여 파라미터 추정 -> Likelihood가 최대화인가?) 
  - [3] 파라미터 추정값 도출

#### 라. EM 알고리즘의 진행 과정

- **혼합 분포 군집모형의 특징**
  - k-평균군집의 절차와 유사하지만 **확률분포를 도입하여 군집을 수행** 한다
  - 군집을 몇 개의 모수로 표현할 수 있으며, 서로 다른 크기나 모양의 군집을 찾을 수 있다
  - EM 알고리즘을 이용한 모수 추정에서 데이터가 커지면 수렴에 시간이 걸릴 수 있다
  - 군집의 크기가 너무 작으면 추정의 정도가 떨어지거나 어려울 수 있다
  - k-평균군집과 같이 **이상치 자료에 민감** 하므로 사전에 조치가 필요하다


## 6. SOM (Self Organizing Map)

#### 가. 개요

- 자가조직화지도(SOM) 알고리즘은 코호넨(kohonen)에 의해 제시, 개발되었으며 코호넨 맵(Kohonen Maps)이라고도 알려져 있다
- **SOM은 비지도 신경망으로 고차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 정렬** 하여 지도의 형태로 형상화한다. 이러한 형상화는 입력 변수의 위치 관계를 그대로 보존한다는 특징이 있다. 다시 말해 실제 공간의 입력 변수가 가까이 있으면, 지도상에도 가까운 위치에 있게 된다

#### 나. 구성

- SOM 모델은 두 개의 인공신경망 층으로 구성되어 있다

- **[1] 입력층(Input layer: 입력벡터를 받는 층)**
  - **입력 변수의 개수와 동일하게 뉴런 수가 존재** 한다
  - 입력층의 자료는 학습을 통하여 경쟁층에 정렬되는데, 이를 지도(map)라 부른다
  - 입력층에 있는 각각의 뉴런은 경쟁층에 있는 각각의 뉴런들과 연결되어 있으며, 이 때 완전연결(fully connected)되어 있다

- **[2] 경쟁층(competitive layer: 2차원 격차(grid)로 구성된 층)**
  - 입력벡터의 특성에 따라 벡터가 한 점으로 클러스터링 되는 층
  - SOM은 경쟁 학습으로 각각의 뉴런이 입력 벡터와 얼마나 가까운가를 계산하여 연결 강도(connection weight)를 반복적으로 재조정하여 학습한다. 이 과정을 거치면서 연결강도는 입력 패턴과 가장 유사한 경쟁층 뉴런이 승자가 된다
  - 입력 층의 표본 벡터에 가장 가까운 프로토타입 벡터를 선택해 BMU(Best-Matching-Unit)라고 하며, 코호넨의 승자 독점의 학습 규칙에 따라 위상학적 이웃(topological neighbors)에 대한 연결 강도를 조정한다
  - 승자 독식 구조로 인해 경쟁층에는 승자 뉴런만이 나타나며, 승자와 유사한 연결 강도를 갖는 입력 패턴이 동일한 경쟁 뉴런으로 배열된다

#### 다. 특징

- 고차원의 데이터를 저차원의 **지도 형태로 형상화** 하기 때문에 시각적으로 이해가 쉽다
- 입력 변수의 위치 관계를 그대로 보존하기 때문에 실제 데이터가 유사하면 지도상에서 가깝게 표현된다
- 이런 특징 때문에 패턴 발견, 이미지 분석 등에서 뛰어난 성능을 보인다
- 역전파(Back propagation) 알고리즘 등을 이용하는 인공신경망과 달리 단 하나의 전방 패스(feed-forward flow)를 사용함으로써 속도가 매우 빠르다. 따라서, 실시간 학습처리를 할 수 있는 모형이다

#### 라. SOM과 신경망 모형의 차이점

- **신경망 모형**
  - 학습 방법
    - 오차역전파법
  - 구성 
    - 입력층, 은닉층, 출력층
  - 기계 학습 방법의 분류
    - 지도학습(Supervised Learning)

- **SOM**
  - 학습방법
    - 경쟁학습방법
  - 구성
    - 입력층, 2차원 격자(grid) 형태의 경쟁층
  - 기계 학습 방법의 분류
    - 비지도학습(Unsupervised Learning)


## 7. 최신 군집분석 기법들

#### 가. iris 데이터를 활용한 기법 확인

- **[1] Hierarchical Clustering**

```{r}
idx <- sample(1:dim(iris)[1], 40)
iris.s <- iris[idx, ]
iris.s$Species <- NULL

hc <- hclust(dist(iris.s), method = "ave")
plot(hc, hang = -1, labels = iris$Species[idx])
```

- **[2] K-means Clustering**
  - 비계층적 군집방법
- 가) 군집화
```{r}
data(iris)
newiris <- iris
newiris$Species <- NULL

kc <- kmeans(newiris, 3)
```

- 나) 결과비교
```{r}
table(iris$Species, kc$cluster)
```
- 다) 군집화 그래프
```{r}
plot(newiris[c("Sepal.Length", "Sepal.Width")], col = kc$cluster)
```


# Chapter 6 - 연관분석 (Association Analysis)

## 1. 연관규칙

#### 가. 연관규칙분석(Association Analysis)의 개념

- 연관성 분석은 흔히 장바구니분석(Market Basket Analysis) 또는 서열분석(Sequence Analysis) 이라고 불린다
- 기업의 데이터베이스에서 상품의 구매, 서비스 등 일련의 거래 또는 사건들 간의 **규칙** 을 발견하기 위해 적용한다
- 장바구니 분석
  - 장바구니에 무엇이 같이 들어 있는지에 대한 분석
- 서열 분석
  - A를 산 다음에 B를 산다
  
#### 나. 연관규칙의 형태

- 조건과 반응의 형태(if-then)로 이루어져 있다
  - 만일 A가 일어나면 B가 일어난다
  - 아메리카노(A)를 마시는 손님 중 10%가 브리우니(B)를 먹는다
  - 샌드위치(A)를 먹는 고객의 30%가 탄산수(B)를 함께 마신다
  
#### 다. 연관규칙의 측도

- 산업의 특성에 따라 지지도, 신뢰도, 향상도 값을 잘 보고 규칙을 선택하야 한다
- **[1] 지지도(support)**
  - 전체 거래 중 항목 A와 항목 B를 동시에 포함하는 거래의 비율로 정의한다  
    지지도 = <img src="https://render.githubusercontent.com/render/math?math=$P(A \cap B) = (A \cap B) / total$">

- **[2] 신뢰도(confidence)**
  - 항목 A를 포함한 거래 중에서 항목 A와 항목 B가 같이 포함될 확률이다
  - 연관성의 정도를 파악할 수 있다  
    신뢰도 = <img src="https://render.githubusercontent.com/render/math?math=$P(A \cap B) / P(A) = support /P(A)$">

- **[3] 향상도(lift)**
  - A 가 구매되지 않았을 때의 품목 B 구매확률에 비해 A가 구매됬을 때의 품목 B의 증가 비이다
  - 연관규칙 A -> B는 품목 A와 품목 B의 구매가 서로 관련이 없는 경우에 향상도가 1이 된다  
    향상도 = <img src="https://render.githubusercontent.com/render/math?math=$P(B|A)/P(B) = P(A \cap B) / (P(A)P(B)) = confidence/P(B)$">
  
#### 라. 연관규칙의 절차

- 최소 지지도보다 큰 집합만을 대상으로 높은 지지도를 갖는 품목 집합을 찾는 것이다
- 처음에는 5%로 잡고 규칙이 충분히 도출 되는지를 보고 다양하게 조절하여 시도한다
- 처움부터 너무 낮은 최소 지지도를 선정하는 것은 많은 리소스가 소모되므로 적절하지 않다
- 절차
  - 최소 지지도 설정 -> 품목 중 최소 지지도를 넘는 품목 분류 -> 2가지 품목 집합 생성 -> 반복적으로 수행해 빈발품목 집합을 찾음
  
#### 마. 연관규칙의 장점과 단점

- **장점**
  - 탐색적인 기법으로 조건 반응으로 표현되는 연관성 분석의 결과를 쉽게 이해할 수 있다
  - 강력한 비목적성 분석기법으로 분석 방향이나 목적이 특별히 없는 경우 목적변수가 없으므로 유용하게 활용 된다
  - 사용이 편리한 분석 데이터의 형태로 거래 내용에 대한 데이터를 변환 없이 그 자체로 이용할 수 있는 간단한 자료 구조를 갖는다
  - 분석을 위한 계산이 간단하다
  
- **단점(개선방안)**
  - 품목수가 증가하면 분석에 필요한 계산은 기하급수적으로 늘어난다
    - 이를 개선하기 위해 유사한 품목을 한 범주로 일반화 한다
    - 연관 규칙의 신뢰도 하한을 새롭게 정의해 실제 관찰되는 빈도가 적은 연관규칙은 제외한다
  - 너무 세분화한 품목을 갖고 연관성 규칙을 찾으면 의미없는 분석이 될 수도 있다
    - 적절히 구분되는 큰 범주로 구분해 전체 분석에 포함시킨 후 그 결과 중에서 세부적으로 연관규칙을 찾는 작업을 수행할 수 있다
  - 거래량이 적은 품목은 당연히 포함된 거래수가 적을 것이고, 규칙 발견 시 제외하기가 쉽다
    - 이런 경우, 그 품목이 관련성을 살펴보고자 하는 중요한 품목이라면 유사한 품목들과 함께 범주로 구성하는 방법 등을 통해 연관성 규칙의 과정에 포함 시킬 수 있다
    
#### 바. 순차패턴(Sequence Analysis)

- 동시에 구매될 가능성이 큰 상품군을 찾아내는 연관성분석에 시간이라는 개념을 포함시켜 순차적으로 구매 가능성이 큰 상품군을 찾아내는 것이다
- 연관성분석에서의 데이터 형태에서 각각의 고객으로부터 발생한 구매시점에 대한 정보가 포함된다


## 2. 기존 연관성 분석의 이슈

- 대용량 데이터에 대한 연관성분석이 불가능하다
- 시간이 많이 걸리거나 기존 시스템에서 실행 시 시스템 다운이 되는 현상이 발생할 수 있다


## 3. 최근 연관성 분석 동향

- 1세대 알고리즘인 Apriori나 2세대인 FP-Growth에서 발전하여 3세대의 FPV를 이용해 메모리를 효율적으로 사용함으로써 SKU 레벨의 연관성 분석을 성공적으로 적용했다
- 거래내역에 포함되어 있는 모든 품목의 개수가 n개 일 때, 품목들의 전체집합(item set)에서 추출할 수 있는 품목 부분집합의 개수는 2^n - 1 (공집합 제외)개다. 그리고 가능한 모든 연관규칙의 개수는 3^n - 2^(n+1)+1 개다
- 이때 모든 가능한 품목 부분집합의 개수를 줄이는 방식으로 작동하는 것이 Apriori 알고리즘이며, 거래내역 안에 포함된 품목의 개수를 줄여 비교하는 횟수를 줄이는 방식이 FP-Growth 알고리즘이다

#### 가. Apriori 알고리즘

- 최소 지지도보다 큰 지지도 값을 갖는 품목의 집합을 빈발항목집합(frequent item set)이라고 한다
- Apriori 알고리즘은 모든 품목집합에 대한 지지도를 전부 계산하는 것이 아니라, **최소 지지도 이상의 빈발항목집합을 찾은 후** 그것들에 대해서만 연관규칙을 계산하는 것이다
- Apriori는 1994년에 발표된 1세대 알고리즘으로 구현과 이해하기가 쉽다는 장점이 있으나, 지지도가 낮은 후보 집합 생성 시 아이템의 개수가 많아지면 계산 복잡도가 증가한다는 문제점을 가지고 있다

#### 나. FP-Growth 알고리즘

- FP-Growth 알고리즘은 **후보 빈발항목집합을 생성하지 않고, FP-Tree(Frequent Pattern Tree)를 만든 후** 분할정복 방식을 통해 Apriori 알고리즘 보다 더 빠르게 빈발항목집합을 추출할 수 있는 방법이다
- **Apriori 알고리즘의 약점을 보완** 하기 위해 고안된 것으로 데이터베이스를 스캔하는 횟수가 적고, 빠른 속도로 분석이 가능하다

## 4. 연관성 분석 활용 방안

- 장바구니 분석의 경우는 실시간 상품추천을 통한 교차판매에 응용
- 순차패턴 분석은 A를 구매한 사람인데 B를 구매하지 않은 경우, B를 추천하는 교차판매 캠페인에 사용

## 5. 연관성 분석 예제

- **[1] 분석 내용**
  - Groceries 데이터셋은 식료품 판매점의 1달 동안의 POS 데이터이며, 총 169개의 제품과 9,835 건의 거래건수를 포함하고 있다
  - 거래내역을 inspect 함수로 확인할 수 있으며, apriori 함수로 최소지지도와 신뢰도는 각각 0.01, 0.3으로 설정한 뒤 연관규칙분석을 실시했다
  
```{r, message = FALSE}
# install.packages("arulesViz")
library(arulesViz) 

data("Groceries") 
inspect(Groceries[1:3])

rules <- apriori(Groceries, parameter = list(support = 0.01, confidence = 0.3))
inspect(sort(rules, by = c("lift"), decreasing = TRUE)[1:20])
```
  
- **[2] 분석 결과**

```{r}
data(Groceries)
inspect(Groceries[1:3])

apriori(Groceries, parameter = list(support = 0.01, confidence = 0.3))
```
  
  - apriori 알고리즘으로 연관규칙분석을 실행한 결과 총 88개의 아이템으로 연관규칙을 만들어냈으며 125개의 Rule이 발견되었다
  - 규칙의 수가 너무 적으면 지지도와 신뢰도를 낮추고, 너무 많으면 지지도와 신뢰도를 높여야 한다
```{r}
inspect(sort(rules, by = c("lift"), decreasing = TRUE)[1:20])
```
  
  - 향상도를 기준으로 내림차순으로 정렬한 후 상위 5개의 규칙을 확인해봤을 때, rhs의 제품만 구매할 확률에 비해 lhs의 제품을 샀을 때 rhs 제품도 구매할 확률이 약 3배 가량 높다(Lift > 3)
  - 따라서 rhs와 lhs 제품들간 결합상품 할인쿠폰 혹은 품목배치 변경 등을 제안할 수 있다
