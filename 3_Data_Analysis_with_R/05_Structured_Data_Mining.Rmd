---
title: "Part 5: Structured Data Mining"
output: rmarkdown::github_document
---

# Table of Contents

- **Chapter 1 - 데이터 마이닝의 개요 (Data Mining Outline)**
  - 데이터 마이닝
  - 데이터 마이닝의 분석 방법
  - 분석 목적에 따른 작업 유형과 기법
  - 데이터 마이닝 추진단계
  - 데이터 마이닝을 위한 데이터 분할
  - 성과 분석
  
- **Chapter 2 - 분류분석 (Classification Analysis)**
  - 분류분석과 예측분석
  - 로지스틱 회귀분석 (Logistic Regression)
  - 의사결정나무
  - 불순도의 여러 가지 측도
  - 의사결정나무 알고리즘
  - 의사결정나무 예시
  
- **Chapter 3 - 앙상블 분석 (Ensemble Analysis)**
  - 앙상블 (Ensemble)

- **Chapter 4 - 인공신경망 분석 (Artificial Neural Network Analysis)**
  - 인공신경만 분석 (ANN)

- **Chapter 5 - 군집분석 (Cluster Analysis)**
  - 군집분석
  - 거리
  - 계층적 군집분석
  - 비계층적 군집분석
  - 혼합 분포 군집 (mixture distribution clustering)
  - SOM (Self Organizing Map)
  - 최신 군집분석 기법들

- **Chapter 6 - 연관분석 (Association Rule)**
  - 연관규칙
  - 기존 연관성 분석의 이슈
  - 최근 연관성 분석 동향
  - 연관성 분석 활용 방안
  - 연관성 분석 예제
  
  
# Chapter 1 - 데이터 마이닝의 개요 (Data Mining Outline)

## 1. 데이터 마이닝

#### 가. 개요

- 데이터 마이닝은 대용량 데이터에서 의미있는 패턴을 파악하거나 예측하여 의사결정에 활용하는 방법이다

#### 나. 통계분석 과의 차이점

- 통계분석은 가설이나 가정에 따른 분석이나 검증을 하지만 **데이터 마이닝** 은 다양한 수리 알고리즘을 이용해 데이터 베이스의 **데이터로부터 의미있는 정보를 찾아내는 방법을 통칭** 한다

#### 다. 종류

- **정보를 찾는 방법론에 따른 종류**
  - 인공지능 (Artificial Intelligence)
  - 의사결정나무 (Decision Tree)
  - K-평균군집화 (K-means clustering)
  - 연관분석 (Association Rule)
  - 회귀분석 (Regression)
  - 로짓분석 (Logit Analysis)
  - 최근접이웃 (Nearest Neighborhood)
 
- **분석대상, 활용목적, 표현방법에 따른 분류**
  - 시각화분석 (Visualization Analysis)
  - 분류 (Classification)
  - 군집화 (Clustering)
  - 포케스팅 (Forecasting)
  
#### 라. 사용분야

- 병원에서 환자 데이터를 이용해서 해당 환자에게 발생 가능성이 높은 병을 예측
- 기존 환자가 응급실에 왔을 때 어떤 조치를 먼저 해야 하는지를 결정
- 고객 데이터를 이용해 해당 고객의 우량/불량을 예측해 대출적격 여부 판단
- 세관 검사에서 입국자의 이력과 데이터를 이용해 관세물품 반입 여부를 예측

#### 마. 데이터 마이닝의 최근 환경

- 데이터 마이닝 도구가 다양하고 체계화되어 환경에 적합한 제품을 선택하여 활용 가능하다
- 알고리즘에 대한 깊은 이해가 없어도 분석에 큰 어려움이 없다
- 분석 결과의 품질은 분석가의 경험과 역량에 따라 차이가 나기 때문에 분석 과제의 복잡성이나 중요도가 높으면 풍부한 경험을 가진 전문가에게 의뢰할 필요가 있다
- 국내에서 데이터 마이닝이 적용된 시기는 1990년대 중반이다
- 2000년대에 비즈니스 관점에서 데이터 마이닝이 CRM의 중요한 요소로 부각되었다
- 대중화를 위해 많은 시도가 있었으나, 통계학 전문가와 대기업 위주로 진행되었다

**기출문제**  
**94. 다음 중 기업이 보유하고 있는 거래 데이터, 고객 데이터 등과 외부 데이터를 포함하는 모든 데이터를 기반으로 새로운 규칙 등을 발견하고 이를 비즈니스 의사결정 등에 정보로 활용하고자 하는 일련의 작업을 무엇이라고 하는가?**

1. 회귀분석  
2. **데이터 마이닝**  
3. 데이터 웨어하우징  
4. 의사결정지원시스템


## 2. 데이터 마이닝의 분석 방법

- **지도학습(Supervised Data Prediction)**
  - 의사결정나무
  - 인공신경망
  - 일반화 선형 모형
  - 회귀분석
  - 로지스틱 회귀분석
  - 사례기반 추론
  - 최근접 이웃법

- **비지도학습(Unsupervised Data Prediction)**
  - OLAP (Online Analytical Processing)
  - 연관성 규칙발견 (Association Rule Discovery, Market Basket)
  - 군집분석 (K-means Clustering)
  - SOM (Self Organizing Map)
  
**기출문제**  
**95. 정형데이터 마이닝 중에서 비지도학습 기법을 사용하여 분석을 수행하였다. 가장 적절한 것은?**  
(a) **고객의 거래 구매 패턴을 분석하여 고객이 구매하지 않은 상품을 추천**  
(b) 우편물에 인쇄된 우편번호 판별 분석을 통해 우편물을 자동으로 분류  
(c) 동일 차종의 수리 보고서 데이터를 분석하여 차량 수리에 소요되는 시간을 예측  
(d) **상품을 구매할 때 그와 유사한 상품을 구매한 고객들의 구매 데이터를 분석하여 제시**

## 3. 분석 목적에 따른 작업 유형과 기법

- **예측 (Predictive Modeling)**
  - **분류 규칙 (Classification)**
    - 가장 많이 사용되는 작업으로 과거의 데이터부로부터 고객특성을 찾아내어 분류모형을 만들어 이를 토대로 새로운 레코드의 결과값을 예측하는 것으로 목표 마케팅 및 고객 신용평가 모형에 활용됨
    - 회귀분석, 판별분석, 신경망, 의사결정나무
    
- **설명 (Descriptive Modeling)**
  - **연관 규칙 (Association Rule)**
    - 데이터 안에 존재하는 항목간의 종속관계를 찾아내는 작업으로, 제품이나 서비스의 교차판매(Cross Selling), 매장진열(Display), 첨부우편(Attached Mailings), 사기적발(Fraud Detection) 등의 다양한 분야에 활용됨
    - <사용기법> 동시발생 매트릭스
  - **연속 규칙 (Sequence)**
    - 연관 규칙에 시간관련 정보가 포함된 형태로, 고객의 구매이력(History) 속성이 반드시 필요하며, 목표 마케팅(Target Marketing)이나 일대일 마케팅(One-to-One Marketing)에 활용됨
    - <사용기법> 동시발생 매트릭스
  - **데이터 군집화 (Clustering)**
    - 고객 레코드들을 유사한 특성을 지닌 몇 개의 소그룹으로 분할하는 작업으로 작접의 특성이 분류규칙(Classification)과 유사하나 분석대상 데이터에 결과값이 없으며, 판촉활동이나 이벤트 대상을 선정하는데 활용됨
    - <사용기법> K-Means Clustering

## 4. 데이터 마이닝 추진단계

#### 가. 1단계: 목적 설정

- 데이터 마이닝을 통해 무엇을 왜 하는지 명확한 목적(이해관계자 모두 동의하고 이해할 수 있는)을 설정한다
- 전문가가 참여해 목적에 따라 사용할 모델과 필요한 데이터를 정의한다

#### 나. 2단계: 데이터 준비

- 고객정보, 거래정보, 상품 마스터정보, 웹로그 데이터, 소셜 네트워크 데이터 등 다양한 데이터를 활용한다
- IT 부서와 사전에 협의하고 일정을 조율하여 데이터 접근 부하에 유의하여야 하며, 필요시 다른 서버에 저장하여 운영에 지장이 없도록 데이터를 준비한다
- 데이터 정제를 통해 데이터의 품질을 보장하고, 필요시 데이터를 보강하여 충분한 양의 데이터를 확보한다

#### 다. 3단계: 가공

- 모델링 목적에 따라 목적 변수를 정의한다
- 필요한 데이터를 데이터 마이닝 스프트웨어에 적용할 수 있는 형식으로 가공한다

#### 라. 4단계: 기법 적용

- 1단계에서 명확한 목적에 맞게 데이터 마이닝 기법을 적용하여 정보를 추출한다

#### 마. 5단계: 검증

- 데이터 마이닝으로 추출된 정보를 검증한다
- 테스트 데이터와 과거 데이터를 활용하여 최적의 모델을 선정한다
- 검증이 완료되면 IT 부서와 협의해 상시 데이터 마이닝 결과를 업무에 적용하고 보고서를 작성항여 추가수익과 투자대비성과(ROI) 등으로 기대효과를 전파한다


## 5. 데이터 마이닝을 위한 데이터 분할

#### 가. 개요

- 모델 평가용 테스트 데이터와 구축용 데이터로 분할하여, 구축용 데이터로 모형을 생성하고 테스트 데이터로 모형이 얼마나 적합한 지를 판단한다

#### 나. 데이터 분할

- **1) 구축용(training data, 50%)**
  - 추정용, 훈련용 데이터라고도 불리며 데이터 마이닝 모델을 만드는데 활용한다
  
- **2) 검정용(validation data, 30%)**
  - 구축된 모형의 과대추정 또는 과소추청을 미세 조정하는 것에 활용한다
  
- **3) 시험용(test data, 20%)**
  - 테스트 데이터나 과거 데이터를 활용하여 모델의 성능을 검증하는데 활용한다
  
- **4) 데이터의 양이 충분하지 않거나 입력 변수에 대한 설명이 충분한 경우**
  - 가) 홀드아웃(hold-out) 방법
    - 주어진 데이터를 랜덤하게 두 개의 데이터로 구분하여 사용하는 방법으로 주로 학습용(training data)과 시험용(test data)로 분리하여 사용한다
  - 나) 교차확인(cross-validation) 방법
    - 주어진 데이터를 k개의 하부집단으로 구분하여, k - 1개의 집단을 학습용으로 나머지는 하부집단으로 검증용으로 설정하여 학습한다
    - k번 반복 측정한 결과를 평ㅇ균낸 값을 최종값으로 한다
    - 주로 10-fold 교차분석을 많이 사용한다
    

## 6. 성과분석

- **Condition Positive + Prediction Positive**
  - True Positive (TP)
- **Condition Positive + Prediction Negative**
  - False Negative (FN)
- **Condition Negative + Prediction Negative**
  - True Negative (TN)
- **Condition Negative + Prediction Positive**
  - False Positive (FP)
 
#### 가. 오분류에 대한 추정치

- **[1] 정분류율(Accuracy)**
  - Accuracy = (TP + TN) / (TP + TN + FP + FN)

- **[2] 오분류율(Error Rate)**
  - Error Rate = (FP + FN) / (TP + TN + FP + FN)
  
- **[3] 특이도(Specificity) = TNR: True Negative Rate**
  - Specificity = TN / (TN + FP)
  
- **[4] 민감도(Sensitivity) = 재현율(Recall) = TPR: True Positive Rate**
  - Sensitivity = TP / (TP + FN)
  
- **[5] 정확도(Precision)**
  - Precision = TP / (TP + FP)
  
- **[6] F1 Score**
  - F1 = 2 x (Precision x Recall) / (Precision + Recall)
  
  
#### 나. ROCR 패키지로 성과분석

- **1) ROC Curve (Receiver Operating Characteristic Curve)**
  - ROC Curve란 가로축을 FPR(False Positive Rate = 1 - TNR(specificity)) 값으로 두고, 세루축을 TPR(Sensitivity) 값으로 두어 시각화한 그래프이다
  - 2진 분류(binary classification)에서 모형의 성능을 평가하기 위해 많이 사용되는 척도이다
  - 그래프가 왼쪽 상단에 가깝게 그려질수록 올바르게 예측한 비율은 높고, 잘못 예측한 비율은 낮음을 의미한다
  - 따라서 **ROC 곡선 아래의 면적을 의미하는 AUROC(Area Under ROC)** 값이 크면 클수록(1에 가까울수록) 모형의 성능이 좋다고 평가한다
  - TPR(True Positive Rate = Sensitivity)
    - 1인 케이스에 대한 1로 예측한 비율
  - FPR(False Positive Rate = 1 - Specificity)
    - 0인 케이스에 대한 1로 잘못 예측한 비율
  - AUROC(Area Under ROC)를 이용한 정확도의 판단 기준
    - **excellent (A)**
      - 0.9 - 1.0
    - **good**
      - 0.8 - 0.9
    - **fair**
      - 0.7 - 0.8
    - **poor**
      - 0.6 - 0.7
    - **fail**
      - 0.5 - 0.6

- **2) ROC Curve와 AUROC의 활용 예시**
  - AUROC = (AR + 1) / 2
  
- **3) R 실습 코드**
  - ROCR 패키지는 binary classification 만 지원가능
  
```{r, message = FALSE, warning = FALSE}
library(rpart)
# install.packages("party")
library(party)
# install.packages("ROCR")
library(ROCR)

x <- kyphosis[sample(1:nrow(kyphosis), nrow(kyphosis), replace = F), ]

x.train <- kyphosis[1:floor(nrow(x) * 0.75), ]
x.evaluate <- kyphosis[floor(nrow(x) * 0.75):nrow(x), ]

x.model <- cforest(Kyphosis ~ Age + Number + Start, data = x.train)

x.evaluate$prediction <- predict(x.model, newdata = x.evaluate)
x.evaluate$correct <- x.evaluate$prediction == x.evaluate$Kyphosis

print(paste("% of predicted classification correct", mean(x.evaluate$correct)))

x.evaluate$probabilities <- 1 - unlist(treeresponse(x.model, newdata = x.evaluate), use.names = F)[seq(1, nrow(x.evaluate) * 2, 2)]
```

  - 그래프 1

```{r}
pred <- prediction(x.evaluate$probabilities, x.evaluate$Kyphosis)
perf <- performance(pred, "tpr", "fpr")
plot(perf, main = "ROC curve", colorize = T)
```

  - 그래프 2
  
```{r}
perf <- performance(pred, "lift", "rpp")
plot(perf, main = "lift curve", colorize = T)
```

#### 다. 이익도표(Lift chart)

- **1) 이익도표의 개념**
  - 이익도표는 분류모형의 성능을 평가하기 위한 척도로, 분류된 관측치에 대해 얼마나 예측이 잘 이루어졌는지를 나타내기 위해 임의로 나눈 각 등급별로 반응검출율, 반응률, 리프트 등의 정보를 산출하여 나타내는 도표이다
  - 2000명의 전체고객 중 381명이 상품을 구매한 경우에 대해 이익도표를 만드는 과정을 예로 들어보면, 먼저 데이터셋의 각 관측치에 대한 예측확률을 내림차순으로 정렬한다
  - 이후 데이터를 10개의 구간으로 나눈 다음 각 구간의 반응율(% response)을 산출한다
  - 또한 기본 향상도(baseline lift)에 비해 반응률이 몇 배나 높은지를 계산하는데 이것을 향상도(Lift)라고 한다
  - 이익도표의 각 등급은 예측확률에 따라 매겨진 순위이기 떄문에, 상위 등급에서는 더 높은 반응률을 보이는 것이 좋은 모형이라고 평가할 수 있다
  
- **2) 이익도표의 활용 예시**
  - 전체 2000명 중 381명이 구매
  - Frequency of "buy"
    - 2000명 중 실제로 구매한 사람
  - % Captured Reponse
    - 반응검출율 = 해당 등급의 실제 구매자 / 전체 구매자
  - % Response
    - 반응률 = 해당 등급의 실제 구매자 / 200명
  - Lift
    - 향상도 = 반응률 / 기본 향상도
    - 좋은 모델이라면 Lift가 빠른 속도로 감소해야 한다
  - 등급별로 향상도가 급격하게 변동할 수록 좋은 모형이라고 할 수 있고, 각 등급별로 향상도가 들쭉날쭉하면 좋은 모형이라고 볼 수 없다
  
#### 참고

- **과적합/과대적합 (Overfitting)**
  - 모형이 학습용 데이터(training data)를 과하게 학습하여, 학습 데이터에 대해서는 높은 정확도를 나타내지만 테스트 데이터 혹은 다른 데이터에 적용할 때는 성능이 떨어지는 현상을 의미한다

- **과소적합 (Underfitting)**
  - 모형이 너무 단순하여 데이터 속에 내제되어 있는 패턴이나 규칙을 제대로 학습하지 못하는 경우를 의히만다
  

**기출문제**  
**1. 오분류표를 사용한 평가 지표 중 아래 설명이 나타내는 지표는 무엇인가?**  
정확도(precision)와 재현율(recall)은 한 지표의 값이 높아지면 다른 지표의 값이 낮아질 가능성이 높은 관계를 지니고 있어 이러한 효과를 보정하여 하나의 지표로 만들어 낸 지표

1. **F1**  
2. 민감도  
3. 특이도  
4. 오즈비  

**97-1. 분류모형을 평가할 때 사용되는 지표로 x축에는 (1 - 특이도(specificity))를 나타내며, y축에는 민감도를 나타내어, 특이도가 어떤 관계를 갖고 변하는지를 이차원 평면상에 표현한 곡선은 무엇인가?**

ROC Curve


# Chapter 2 - 분류분석 (Classification Analysis)

## 1. 분류분석과 예측분석

#### 가. 분류분석의 정의

- 데이터가 어떤 그룹에 속하는지 예측하는데 사용되는 기법이다
- 클러스터링과 유사하지만, 분류분석은 각 그룹이 정의되어 있다
- 교사학습(supervised learning)에 해당하는 예측기법이다

#### 나. 예측분석의 정의

- 시계열 분석 처럼 시간에 따른 값 두 개만을 이ㅛㅇ해 앞으로의 매출 또는 온도 등을 예측하는 것
- 모델링을 하는 입력 데이터가 어떤 것인지에 따라 특성이 다르다
- 여러 개의 다양한 설명변수(독립변수)가 아닌, 한 개의 설명변수로 생각하면 된다

#### 다. 분류분석, 예측분석의 공통점과 차이점

- **공통점**
  - 레코드의 특정 속성의 값을 미리 알아맞히는 점이다

- **차이점**
  - 가) 분류
    - 레코드(튜플)의 **범주형 속성** 의 값을 알아맞히는 것이다
  - 나) 예측
    - 레코드(튜플)의 **연속형 속성** 의 값을 알아맞히는 것이다

#### 라. 분류, 예측의 예

- **분류**
  - 가) 학생들의 국어, 영어, 수학 점수를 통해 내신등급을 알아맞히는 것
  - 나) 카드회사에서 회원들의 가입 정보를 통해 1년 후 신용등급을 알아맞히는 것
  
- **예측**
  - 가) 학생들의 여러 가지 정보를 입력하여 수능점수를 알아맞히는 것
  - 나) 카드회사 회원들의 가입정보를 통해 연 매출액을 알아맞히는 것

#### 마. 분류 모델링

- 신용평가모형(우량, 불량)
- 사기방지모형(사기, 정상)
- 이탈모형(이탈, 유지)
- 고객세분화(VVIP, VIP, GOLD, SILVER, BRONZE)

#### 바. 분류 기법

- 회귀분석, 로지스틱 회귀분석(Logistic Regression)
- **의사결정나무(Decision Tree)**, CART(Classification and Regression Tree), C5.0
- 베이지안 분류(Bayesian Classification), Naive Bayesian
- **인공신경망(ANN, Artificial Neural Network)**
- 지지도 벡터기계(SVM, Support Vector Machine)
- k 최근접 이웃(KNN, K-Nearest Neighborhood)
- 규칙기반의 분류와 사례기반추론(Case-Based Reasoning)


## 2. 로지스틱 회귀분석 (Logistic Regression)

- 반응변수가 범주형인 경우에 적용되는 회귀분석모형이다
- 새로운 설명변수(또는 예측변수)가 주어질 때 반응변수의 각 범주(또는 집단)에 속할 확률이 얼아민지를 추정(예측모형)하여, 추정 확률을 기준치에 따라 분류하는 목적(분류모형)으로 활용된다
- 이때 모형의 적합을 통해 추정된 확률을 사후확률(Posterior Probability)이라고 한다  
  log[P(y)/(1 - P(y))] - alpha + B1 x1 + ... + Bk xk  
  P(y) = P(y = 1 | x), x = (x1, ..., xk)  

- exp(B1)의 의미는 나머지 변수(x1, ..., xk)가 주어질 때, x1이 한 단위 증가할 때마다 성공(Y = 1)의 오즈가 몇배 증가하는지를 타나내는 값이다  
  P(y) = [exp(alpha + B1 x1 + ... + Bk xk)] / [1 + exp(alpha + B1 x1+ ... + Bk xk)]  
       = 1 / [1 + exp[-(alpha + B1 x1 + ... + Bk xk)]]  

- 위 식은 다중로지스틱 회귀모형이며, 그래프잉 형태는 설명변수가 한 개(x1)인 경우 해당 회귀 계수 B1의 부호에 따라 S자 모양(B1 > 0) 또는 역 S자 모양 (B1 < 0)을 가진다  
- 표준 로지스틱 분포의 누적분포함수(c.d.f)를 F(x)라 할 때  
   P(y) = F(alpha + B1 x1 + ... + Bk xk)    
   위 식과 동일한 표현이며, 표준 로지스틱 분포의 누적분포함수로 성공의 확률을 추정한다
- <참고>
- **오즈비(odds ratio)**
  - 오즈(odds)는 성공할 확률이 실패할 확률의 몇배인지를 타나내는 확률이다
  - 오즈비(odds ratio)는 오즈의 비율이다
  - ex)
    - 성공확률1: 0.8
    - 실패확률1: 0.2
      - 오즈: 0.8/0.2 = 4
    - 성공확률2: 0.1
    - 실패확률2: 0.9
      - 오즈: 0.1/0.9 = 1/9
    - 오즈비
      - 4/(1/9) = 36
- 선형회귀분석과 로지스틱 회귀분석의 비교
  - **선형회귀분석**
    - 종속변수
      - 연속형 변수
    - 계수 추정법
      - 최소제곱법
    - 모형 검정
      - F 검정, T 검정
  - **로지스틱 회귀분석**
    - 종속변수
      - (0, 1)
    - 계수 추정법
      - 최대 우도 추정법
    - 모형 검정
      - 카이제곱 검정(X^2 test)

- <참고>
- **최대우도 추정법(MLE: Maximum Likelihood Estimation)**
  - 모수가 미지의 theta인 확률분포에서 뽑은 표본(관측치) x들을 바탕으로 theta를 추정하는 기법
  - 우도(likelihood)는 이미 주어진 표본 x들에 비추어봤을 때 모집단의 모수 theta에 대한 추정이 그럴듯한 정도를 말한다
  - 우도 L(theta | x)는 theta가 전제되었을 때 표본 x가 등장할 확률인 p(x | theta)에 비례한다

- glm() 함수를 활용하여 로지스틱 회귀분석을 실행한다
- R 코드
  - glm(종속변수 ~ 독립변수1 + ... + 독립변수k, family = binomial, data = 데이터셋명)


## 3. 의사결정나무

#### 가. 정의

- 의사결정나무는 분류함수를 의사결정 규칙으로 이뤄진 **나무 모양으로 그리는 방법** 이다
- 나무구조는 연속적으로 발생하는 의사결정 문제를 **시각화** 해 의사결정이 이뤄지는 시점과 성과를 한눈에 볼 수 있게 한다
- 계산결과가 의사결정나무에 직접 나타나기 떄문에 해석이 간편하다
- 의사결정나무는 주어진 **입력값에 대하여 출력값을 예측하는 모형** 으로 분류나무와 회귀나무 모형이 있다
- <참고>
- 뿌리마디(root node)
  - 시작되는 마디로 전체 자료를 포함
- 자식마디(child node)
  - 하나의 마디로부터 분리되어 나간 2개 이상의 마디들
- 부모마디(parent node)
  - 주어진 마디의 상위 마디
- 끝마디(terminal node)
  - 자식마디가 없는 마디
- 중간마디(internal node)
  - 부모마디와 자식마디가 모두 있는 마디
- 가지(branch)
  - 뿌리마디로부터 끝마디까지 연결된 마디들
- 깊이(depth)
  - 뿌리마디부터 끝마디까지의 중간 마디들의 수


#### 나. 예측력과 해석력

- 기대 집단의 사람들 중 가장 많은 반응을 보일 **고객의 유치방안을 예측** 하고자 하는 경우에는 **예측력에 치중** 한다
- 신용평가에서는 심사 결과 부적격 판정이 나온 경우 고객에게 부적격 **이류를 설명** 해야하므로 **해석력에 치중** 한다


#### 다. 의사결정나무의 활용

- **[1] 세분화**
  - 데이터를 비슷한 특성을 갖는 몇 개의 그룹으로 분할해 그룹별 특성을 발견하는 것이다
- **[2] 분류**
  - 여러 예측변수들에 근거해 관측개체의 목표변수 범주를 몇 개의 등급으로 분류하고자 하는 경우에 사용하는 기법이다
- **[3] 예측**
  - 자료에서 규칙을 찾아내고 이를 이용해 미래의 사건을 예측하고자 하는 경우이다
- **[4] 차원축소 및 변수선택**
  - 매우 많은 수의 예측변수 중에서 목표변수에 큰 영향을 미치는 변수들을 골라내고자 하는 경우에 사용하는 기법이다
- **[5] 교호작용(interaction)효과의 파악**
  - 여러 개의 예측변수들을 결합해 목표변수에 작용하는 규칙을 파악하고자 하는 경우
  - 범주의 병합 또는 연속형 변수의 이산화
    = 범주형 목표변수의 범주를 소수의 몇 개로 병합하거나 연속형 목표변수를 몇 개의 등급으로 이산화 하고자 하는 경우이다


#### 라. 의사결정나무의 특징

- **<장점>**
  - 결과를 누구에가나 설명하기 용이하다
  - 모형을 만드는 방법이 계산적으로 복잡하지 않다
  - 대용량 데이터에서도 빠르게 만들 수 있다
  - 비정상 잡음 데이터에 대해서도 민감함이 없이 분류할 수 있다
  - 한 변수와 상관성이 높은 다른 불필요한 변수가 있어도 크게 영향을 받지 않는다
  - 설명변수나 목표변수에 수치형변수와 범주형변수를 모두 사용 가능하다
  - 모형 분류 정확도가 높다

- **<단점>**
  - 새로운 자료에 대한 과대적합이 발생할 가능성이 높다
  - 분류 경계선 부근의 자료값에 대해서 오차가 크다
  - 설명변수 간의 중요도를 판단하기 쉽지 않다


#### 마. 의사결정나무의 분석 과정

- 의사결정나무의 형성과정은 크게 성장(growing), 가지치기(pruning), 타당성 평가, 해석 및 예측으로 이루어진다

- **[1] 성장 단계**
  - 각 마디에서 적절한 최적의 분리규칙(splitting rule)을 찾아서 나무를 성장시키는 과정으로 적절한 정지규칙(stopping rule)을 만족하면 중단한다
- **[2] 가지치기 단계**
  - 오차를 크게 할 위험이 높거나 부적절한 추론규칙을 가지고 있는 가지 또는 불필요한 가지를 제거하는 단계이다
- **[3] 타당성 평가 단계**
  - 이익도표(gain chart), 위험도표(risk chart), 혹은 시험자료를 이용하여 의사결정나무를 평가하는 단계이다
- **[4] 해석 및 예측 단계**
  - 구축된 나무모형을 해석하고 예측모형을 설정한 후 예측에 적용하는 단계이다
  

#### 바. 나무의 성장

- 훈련자료(xi, yi), i = 1, 2, ..., n 으로 나타내자. 여기서 xi = (xi1, ..., xip) 이다
- 나무모형의 성장과정은 x들로 이루어진 입력 공간을 재귀적(recurrent)으로 분할하는 과정이다  
  <img src="https://render.githubusercontent.com/render/math?math=$R_1(j, A) = x_j \in A$">  
  <img src="https://render.githubusercontent.com/render/math?math=$R_2(j, A^c) = x_j \in A^c$">

- **[1] 분리 규칙(splitting rule)**
  - 분리 변수(split variable)가 연속형인 경우
    - A = xj <= s
  - 분리변수가 범주형 {1, 2, 3, 4} 인 경우
    - A = 1, 2, 4 와 A^c = 3 으로 나눌 수 있다
  - 최적 분할의 결정은 불순도 감소량을 가장 크게 하는 분할이다  
    <img src="https://render.githubusercontent.com/render/math?math=$\Delta i(t) = i(t) - p_L i(t_L) - p_R i(t_R), i(t) = \Sigma_{i} (y_i - \overline{y}_{t})^2$">
  - 각 단계에서 최적 분리기준에 의한 분할을 찾은 다음 각 분할에 대하여도 동일한 과정을 반복한다

- **[2] 분리 기준(splitting critertion)**
  - 이산형 목표변수
    - **카이제곱 통계량 p 값**
      - p 값이 가장 작은 예측변수와 그 때의 최적분리에 의해서 자식마디를 생성
    - **지니 지수**
      - 지니 지수를 감소시켜주는 예측변수와 그 때의 최적분리에 의해서 자식마디를 선택
    - **엔트로피 지수**
      - 엔트로피 지수가 가장 작은 예측 변수와 이 때의 최적분리에 의해 자식마디를 형성
  - 연속형 목표변수
    - **분산분석에서 F 통계량**
      - p 값이 가장 작은 예측변수와 그 때의 최적분리에 의해서 자식마디를 형성
    - **분산의 감소량**
      - 분산의 감소량을 최대화 하는 기준의 최적분리에 의해서 자식마디를 형성
      
- **[3] 정지 규칙(stopping rule)**
  - 더 이상 분리가 일어나지 않고, 현재의 마디가 끝마디가 되도록 하는 규칙이다
  - 정지기준(stopping criterion)
    - 의사결정나무의 깊이(depth)를 지정, 끝마디의 레코드 수의 최소 개수를 지정한다
    

#### 사. 나무의 가지치기(Pruning)
  - 너무 큰 나무모형은 자료를 과대적합하고 너무 작은 나무모형은 과소적합할 위험이 있다
  - 나무의 크기를 모형의 복잡도로 볼 수 있으며 최적의 나무 크기는 자료로부터 추정하게 된다
  - 일반적으로 사용되는 방법은 마디에 속하는 **자료가 일정 수 (가령 5)이하** 일 떄 분할을 정지하고 **비용-복잡도 가지치기(cost complexity pruning)를 이용** 하여 성장시킨 나무를 가지치기하게 된다


## 4. 불순도의 여러 가지 측도

- 목표변수가 범주형 변수인 의사결정나무의 분류규칙을 선택하기 위해서는 카이제곱 통계량, 지니 지수, 엔트로피 지수를 활용한다
- **[1] 카이제곱 통계량**
  - 카이제곱 통계량은 각 셀에 대한 (실제도수 - 기대도수)^2 / 기대도수 의 합으로 구할 수 있다
  - 기대도수 = 열의합계 x 합의합계 / 전체합계  
    <img src="https://render.githubusercontent.com/render/math?math=$\chi^2 = \Sigma_{i = 1}^{k} (O_i - B_i)^2 / B_i$"> (k: 범주의 수, Oi = 실제도수, Bi = 기대도수)
    
- **[2] 지니 지수**
  - 노드의 불순도를 나타내는 값이다
  - 지니지수의 값이 클수록 이질적(Diverse)이며 순수도(Purity)가 낮다고 볼 수 있다  
    <img src="https://render.githubusercontent.com/render/math?math=$Gini(T) = 1 - \Sigma_{l = 1}^{k} p_l ^2$">

- **[3] 엔트로피 지수**
  - 열역학에서 쓰는 개념으로 무질서 정도에 대한 측도이다
  - 엔트로피 지수의 값이 클수록 순수도(Purity)가 낮다고 볼 수 있다
  - 엔트로피 지수가 가장 작은 예측 변수와 이때의 최적분리 규칙에 의해 자식마디를 형성하다  
    <img src="https://render.githubusercontent.com/render/math?math=$Entropy(T) = -(\Sigma_{l = 1}^{k} p_l \ log_2 \ p_l)$">
    

## 5. 의사결정나무 알고리즘

#### 가. CART(Classification and Regression Tree)

- 앞에서 설명한 방식의 가장 많이 활용되는 의사결정나무 알고리즘으로 불순도의 측도로 출력(목적) 변수가 범주형일 경우 지니수를 이용, 연속형인 경우 반산을 이용한 이진분리(binary split)를 사용한다
- 개별 입력변수 뿐만 아니라 입력변수들의 선형결합들 중에서 최적의 분리를 찾을 수 있다

#### 나. C4.5와 C5.0

- CART와는 다르게 각 마디에서 다지분리(multiple split)가 가능하며 범주형 입력변수에 대헤서는 범주의 수만큼 분리가 일어난다
- 불순도의 측도로는 엔트로피 지수를 사용한다

#### 다. CHAID(Chi-squared Automatic Interaction Detection)

- 가지치기를 하지 않고 적당한 크기에서 나무모형의 성장을 중지하며 입력변수가 반드시 범주형 변수이어야 한다
- 불순도의 축도로는 카이제곱 통계량을 사용한다


## 6. 의사결정나무 예시

#### 가. party 패키지를 이용한 의사결정 나무

- party 패키지는 의사결정나무를 사용하기 편한 다양한 분류 패키지 중 하나이다
- 분실값을 잘 처리하지 못하는 문제를 갖고 있는 것이 단점이다
- tree 에 투입된 데이터가 표시가 되지 않거나 predict 가 실패하는 경우 문제가 발생할 수 있다
- **[1] iris data 를 이용한 분석**
  - iris data 의 70% 는 training data, 30% 는 test data 로 생성한다

```{r}
idx <- sample(2, nrow(iris), replace = TRUE, prob = c(0.7, 0.3))

train.data <- iris[idx == 2, ]
test.data <- iris[idx == 1, ]
```

- **[2] train.data 를 이용하여 모형생성**

```{r, message = FALSE}
# install.packages("party")
library(party)
# library(sandwich)

iris.tree <- ctree(Species ~ ., data = train.data)

plot(iris.tree)
plot(iris.tree, type = "simple")
```

- **[3] 예측된 데이터와 실제 데이터의 비교**

```{r}
table(predict(iris.tree), train.data$Species)
```

- **[4] test data 를 적용하여 정확성 확인**

```{r}
test.pre <- predict(iris.tree, newdata = test.data)
table(test.pre, test.data$Species)
```